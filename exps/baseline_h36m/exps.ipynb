{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Code to run in bash console\n",
    "# cd exps/baseline_h36m\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import os, sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from config import config\n",
    "\n",
    "import model as models\n",
    "from datasets.h36m import H36MDataset\n",
    "from utils.logger import get_logger, print_and_log_info\n",
    "from utils.pyt_utils import link_file, ensure_dir\n",
    "from datasets.h36m_eval import H36MEval\n",
    "\n",
    "from custom_test import test\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# cuda setting to make result deterministic\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--exp-name', type=str, default=None, help='=exp name')\n",
    "parser.add_argument('--seed', type=int, default=888, help='=seed')\n",
    "parser.add_argument('--temporal-only', action='store_true', help='=temporal only')\n",
    "parser.add_argument('--layer-norm-axis', type=str, default='spatial', help='=layernorm axis')\n",
    "# default is False for 'store_true'\n",
    "parser.add_argument('--with-normalization', action='store_true', help='=use layernorm')\n",
    "parser.add_argument('--spatial-fc', action='store_true', help='=use only spatial fc')\n",
    "parser.add_argument('--num', type=int, default=64, help='=num of blocks')\n",
    "parser.add_argument('--weight', type=float, default=1., help='=loss weight')\n",
    "\n",
    "# pass argument without command line\n",
    "import shlex\n",
    "argString = '--seed 888 --exp-name baseline.txt --layer-norm-axis spatial --with-normalization --num 48'\n",
    "args = parser.parse_args(shlex.split(argString))\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "acc_log = open(args.exp_name, 'a')\n",
    "torch.manual_seed(args.seed)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "config.motion_fc_in.temporal_fc = args.temporal_only\n",
    "config.motion_fc_out.temporal_fc = args.temporal_only\n",
    "config.motion_mlp.norm_axis = args.layer_norm_axis\n",
    "config.motion_mlp.spatial_fc_only = args.spatial_fc\n",
    "config.motion_mlp.with_normalization = args.with_normalization\n",
    "config.motion_mlp.num_layers = args.num\n",
    "\n",
    "# config.motion_rnn.with_normalization = args.with_normalization\n",
    "\n",
    "acc_log.write(''.join('Seed : ' + str(args.seed) + '\\n'))\n",
    "\n",
    "def get_dct_matrix(N):\n",
    "\tdct_m = np.eye(N)\n",
    "\tfor k in np.arange(N):\n",
    "\t\tfor i in np.arange(N):\n",
    "\t\t\tw = np.sqrt(2 / N)\n",
    "\t\t\tif k == 0:\n",
    "\t\t\t\tw = np.sqrt(1 / N)\n",
    "\t\t\tdct_m[k, i] = w * np.cos(np.pi * (i + 1 / 2) * k / N)\n",
    "\tidct_m = np.linalg.inv(dct_m)\n",
    "\treturn dct_m, idct_m\n",
    "\n",
    "# size: (1,T,T)\n",
    "dct_m,idct_m = get_dct_matrix(config.motion.h36m_input_length_dct)\n",
    "dct_m = torch.tensor(dct_m).float().cuda().unsqueeze(0)\n",
    "idct_m = torch.tensor(idct_m).float().cuda().unsqueeze(0)\n",
    "\n",
    "def update_lr_multistep(nb_iter, total_iter, max_lr, min_lr, optimizer) :\n",
    "\tif nb_iter > 30000:\n",
    "\t\tcurrent_lr = min_lr\n",
    "\telse:\n",
    "\t\tcurrent_lr = max_lr\n",
    "\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\tparam_group[\"lr\"] = current_lr\n",
    "\n",
    "\treturn optimizer, current_lr\n",
    "\n",
    "def gen_velocity(m):\n",
    "\tdm = m[:, 1:] - m[:, :-1]\n",
    "\treturn dm\n",
    "\n",
    "def train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, total_iter, max_lr, min_lr) :\n",
    "\n",
    "\tif config.pre_dct:\n",
    "\t\tb,n,c = h36m_motion_input.shape\n",
    "\t\th36m_motion_input_ = h36m_motion_input.clone()\n",
    "\t\th36m_motion_input_ = torch.matmul(dct_m[:, :, :config.motion.h36m_input_length], h36m_motion_input_.cuda())\n",
    "\telse:\n",
    "\t\th36m_motion_input_ = h36m_motion_input.clone()\n",
    "\n",
    "\tmotion_pred = model(h36m_motion_input_.cuda())\n",
    "\n",
    "\tif config.post_dct:\n",
    "\t\tmotion_pred = torch.matmul(idct_m[:, :config.motion.h36m_input_length, :], motion_pred)\n",
    "\n",
    "\tif config.residual_output:\n",
    "\t\toffset = h36m_motion_input[:, -1:].cuda()\n",
    "\t\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length] + offset\n",
    "\telse:\n",
    "\t\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length]\n",
    "\n",
    "\t# calc losses\n",
    "\tb,n,c = h36m_motion_target.shape\n",
    "\tmotion_pred = motion_pred.reshape(b,n,22,3).reshape(-1,3)\n",
    "\th36m_motion_target = h36m_motion_target.cuda().reshape(b,n,22,3).reshape(-1,3)\n",
    "\tloss = torch.mean(torch.norm(motion_pred - h36m_motion_target, 2, 1))\n",
    "\t# add position loss and velocity loss\n",
    "\tif config.use_relative_loss:\n",
    "\t\tmotion_pred = motion_pred.reshape(b,n,22,3)\n",
    "\t\tdmotion_pred = gen_velocity(motion_pred)\n",
    "\t\tmotion_gt = h36m_motion_target.reshape(b,n,22,3)\n",
    "\t\tdmotion_gt = gen_velocity(motion_gt)\n",
    "\t\tdloss = torch.mean(torch.norm((dmotion_pred - dmotion_gt).reshape(-1,3), 2, 1))\n",
    "\t\tloss = loss + dloss\n",
    "\telse:\n",
    "\t\tloss = loss.mean()\n",
    "\n",
    "\twriter.add_scalar('Loss/angle', loss.detach().cpu().numpy(), nb_iter)\n",
    "\n",
    "\t# reset gradients\n",
    "\toptimizer.zero_grad()\n",
    "\t# compute gradients by backpropagation\n",
    "\tloss.backward()\n",
    "\t# update params\n",
    "\toptimizer.step()\n",
    "\toptimizer, current_lr = update_lr_multistep(nb_iter, total_iter, max_lr, min_lr, optimizer)\n",
    "\twriter.add_scalar('LR/train', current_lr, nb_iter)\n",
    "\n",
    "\treturn loss.item(), optimizer, current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "siMLPe_RNN(\n",
      "  (rnn): Seq2SeqLSTM(\n",
      "    (endecoder): LSTM(66, 82, batch_first=True)\n",
      "    (fc0): Linear(in_features=82, out_features=66, bias=True)\n",
      "    (spatial_norm): Identity()\n",
      "  )\n",
      ")\n",
      "\n",
      "Total count of parameters: 54678\n",
      "Residual output?  False\n",
      "Use DCT?  False\n",
      "recursive_residual?  False\n",
      "Using recursive residual? False\n",
      "Using LayerNorm? True\n"
     ]
    }
   ],
   "source": [
    "if config.model == 'siMLPe':\n",
    "\tmodel = models.siMLPe(config)\n",
    "elif config.model == 'siMLPe_RNN':\n",
    "\tmodel = models.siMLPe_RNN(config, rnn_state_size=int(config.motion.dim*1.25), rnn_layers=config.motion_rnn.num_layers, num_blocks=config.motion_rnn.num_blocks)\n",
    "elif config.model == 'Seq2SeqGRU':\n",
    "\tmodel = models.Seq2SeqGRU(config, state_size=int(config.motion.dim*1.25), num_layers=config.motion_rnn.num_layers)\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print()\n",
    "print(\"Total count of parameters:\",total_params)\n",
    "print(\"Residual output? \",config.residual_output)\n",
    "print(\"Use DCT? \",config.pre_dct)\n",
    "print(\"recursive_residual? \",config.motion_rnn.recursive_residual)\n",
    "print(\"Using recursive residual?\",config.motion_rnn.recursive_residual)\n",
    "print(\"Using LayerNorm?\",config.motion_mlp.with_normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/home/gjsk/siMLPe/exps/baseline_h36m/log/log_last.log': File exists\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "# dataset = (T-by-C x_in, N-by-C x_out)\n",
    "config.motion.h36m_target_length = config.motion.h36m_target_length_train\n",
    "dataset = H36MDataset(config, 'train', config.data_aug)\n",
    "\n",
    "# separate into batches (input, target) with size (batch_size,T,C) and (batch_size,N,C)\n",
    "shuffle = True\n",
    "sampler = None\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size,\n",
    "\t\t\t\t\t\tnum_workers=config.num_workers, drop_last=True,\n",
    "\t\t\t\t\t\tsampler=sampler, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "eval_config = copy.deepcopy(config)\n",
    "eval_config.motion.h36m_target_length = eval_config.motion.h36m_target_length_eval\n",
    "eval_dataset = H36MEval(eval_config, 'test')\n",
    "\n",
    "shuffle = False\n",
    "sampler = None\n",
    "# separate into batches (input, target) with size (batch_size,T=50,K,3) and (batch_size,N=25,K,3)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=128,\n",
    "\t\t\t\t\t\tnum_workers=1, drop_last=False,\n",
    "\t\t\t\t\t\tsampler=sampler, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "\t\t\t\t\t\t\t lr=config.cos_lr_max,\n",
    "\t\t\t\t\t\t\t weight_decay=config.weight_decay)\n",
    "\n",
    "ensure_dir(config.snapshot_dir)\n",
    "logger = get_logger(config.log_file, 'train')\n",
    "link_file(config.log_file, config.link_log_file)\n",
    "\n",
    "print_and_log_info(logger, json.dumps(config, indent=4, sort_keys=True))\n",
    "\n",
    "# continue training from a checkpoint\n",
    "if config.model_pth is not None :\n",
    "\tstate_dict = torch.load(config.model_pth)\n",
    "\tmodel.load_state_dict(state_dict, strict=True)\n",
    "\tprint_and_log_info(logger, \"Loading model path from {} \".format(config.model_pth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(23.9), np.float64(44.1), np.float64(75.4), np.float64(87.4), np.float64(105.5), np.float64(119.4), np.float64(128.3), np.float64(133.6)]\n",
      "[0.1, -0.3, -0.7, -0.8, -1.9, -2.2, -3.3, -3.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), config\u001b[38;5;241m.\u001b[39msnapshot_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/model-iter-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(nb_iter \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     30\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 31\u001b[0m acc_tmp \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(acc_tmp)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m([\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(acc_tmp[i]\u001b[38;5;241m-\u001b[39mbaseline_results[i]),\u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m)])\n",
      "File \u001b[0;32m~/siMLPe/exps/baseline_h36m/custom_test.py:99\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(config, model, dataloader)\u001b[0m\n\u001b[1;32m     96\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     98\u001b[0m pbar \u001b[38;5;241m=\u001b[39m dataloader\n\u001b[0;32m---> 99\u001b[0m m_p3d_h36 \u001b[38;5;241m=\u001b[39m \u001b[43mregress_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_used_xyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_p3d_h36\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m ret \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mmotion\u001b[38;5;241m.\u001b[39mh36m_target_length):\n",
      "File \u001b[0;32m~/siMLPe/exps/baseline_h36m/custom_test.py:55\u001b[0m, in \u001b[0;36mregress_pred\u001b[0;34m(model, pbar, num_samples, joint_used_xyz, m_p3d_h36)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpre_dct:\n\u001b[1;32m     53\u001b[0m     motion_input_ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(dct_m[:, :, :config\u001b[38;5;241m.\u001b[39mmotion\u001b[38;5;241m.\u001b[39mh36m_input_length], motion_input_)\n\u001b[0;32m---> 55\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmotion_input_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpost_dct:\n\u001b[1;32m     58\u001b[0m     output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(idct_m[:, :config\u001b[38;5;241m.\u001b[39mmotion\u001b[38;5;241m.\u001b[39mh36m_input_length, :], output)[:, :step, :]\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/siMLPe/exps/baseline_h36m/model.py:270\u001b[0m, in \u001b[0;36msiMLPe_RNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 270\u001b[0m     _x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _x\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/siMLPe/exps/baseline_h36m/model.py:243\u001b[0m, in \u001b[0;36mSeq2SeqLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# Decoder\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     frame_delta, (hidden_states, cell_states) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendecoder(decoder_input, (hidden_states, cell_states))\n\u001b[0;32m--> 243\u001b[0m     frame_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_delta\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, 1, C]\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# frame_delta = self.fc1(frame_delta)  # [B, 1, C]\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# frame_delta = self.fc2(frame_delta)  # [B, 1, C]\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;66;03m# frame_delta = self.fc3(frame_delta)  # [B, 1, C]\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;66;03m# frame_delta = self.fc4(frame_delta)  # [B, 1, C]\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmotion_rnn\u001b[38;5;241m.\u001b[39mrecursive_residual:\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;66;03m# Residual method 1 (recursive residual; same as in 2017 Martinez paper):\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1915\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[0;32m-> 1915\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1917\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_iter = 0\n",
    "avg_loss = 0\n",
    "avg_lr = 0\n",
    "current_lr = config.cos_lr_max\n",
    "\n",
    "config.save_every = 100\n",
    "config.cos_lr_total_iters = 10000\n",
    "baseline_results = [23.8,44.4,76.1,88.2,107.4,121.6,131.6,136.6]\n",
    "\n",
    "# about 1 min per 1000 iterations\n",
    "while (nb_iter + 1) < config.cos_lr_total_iters:\n",
    "\n",
    "\tfor (h36m_motion_input, h36m_motion_target) in dataloader:\n",
    "\n",
    "\t\tloss, optimizer, current_lr = train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, config.cos_lr_total_iters, config.cos_lr_max, config.cos_lr_min)\n",
    "\t\tavg_loss += loss\n",
    "\t\tavg_lr += current_lr\n",
    "\n",
    "\t\tif (nb_iter + 1) % config.print_every ==  0 :\n",
    "\t\t\tavg_loss = avg_loss / config.print_every\n",
    "\t\t\tavg_lr = avg_lr / config.print_every\n",
    "\n",
    "\t\t\tprint_and_log_info(logger, \"Iter {} Summary: \".format(nb_iter + 1))\n",
    "\t\t\tprint_and_log_info(logger, f\"\\t lr: {avg_lr} \\t Training loss: {avg_loss}\")\n",
    "\t\t\tavg_loss = 0\n",
    "\t\t\tavg_lr = 0\n",
    "\n",
    "\t\tif (nb_iter + 1) % config.save_every ==  0 :\n",
    "\t\t\ttorch.save(model.state_dict(), config.snapshot_dir + '/model-iter-' + str(nb_iter + 1) + '.pth')\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tacc_tmp = test(eval_config, model, eval_dataloader)\n",
    "\t\t\tprint(acc_tmp)\n",
    "\t\t\tprint([round(float(acc_tmp[i]-baseline_results[i]),2) for i in range(8)])\n",
    "\t\t\tacc_log.write(''.join(str(nb_iter + 1) + '\\n'))\n",
    "\t\t\tline = ''\n",
    "\t\t\tfor ii in acc_tmp:\n",
    "\t\t\t\tline += str(ii) + ' '\n",
    "\t\t\tline += '\\n'\n",
    "\t\t\tacc_log.write(''.join(line))\n",
    "\t\t\tmodel.train()\n",
    "\n",
    "\t\tif (nb_iter + 1) == config.cos_lr_total_iters :\n",
    "\t\t\tbreak\n",
    "\t\tnb_iter += 1\n",
    "\tprint(\"Iter number:\",nb_iter)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "arr0 = Rearrange('b n d -> b d n')\n",
    "arr1 = Rearrange('b d n -> b n d')\n",
    "\n",
    "nb_iter = 0\n",
    "avg_loss = 0\n",
    "avg_lr = 0\n",
    "\n",
    "(h36m_motion_input, h36m_motion_target) = next(iter(dataloader))\n",
    "\n",
    "# loss, optimizer, current_lr = train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, config.cos_lr_total_iters, config.cos_lr_max, config.cos_lr_min)\n",
    "# train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, total_iter, max_lr, min_lr)\n",
    "total_iter, max_lr, min_lr = config.cos_lr_total_iters, config.cos_lr_max, config.cos_lr_min\n",
    "\n",
    "# DCT\n",
    "b,n,c = h36m_motion_input.shape\n",
    "h36m_motion_input_ = h36m_motion_input.clone()\n",
    "h36m_motion_input_ = torch.matmul(dct_m[:, :, :config.motion.h36m_input_length], h36m_motion_input_.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model as models\n",
    "# model = models.siMLPe_RNN(config, rnn_state_size=int(config.motion.dim*1.5), rnn_layers=config.motion_rnn.num_layers, num_blocks=config.motion_rnn.num_blocks).cuda()\n",
    "test_model = models.Seq2SeqGRU_simple(config, state_size=int(config.motion.dim*1.5), num_layers=config.motion_rnn.num_layers).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "# motion_pred = model(h36m_motion_input_.cuda())\n",
    "motion_feats = h36m_motion_input_.cuda()\n",
    "\n",
    "x = motion_feats\n",
    "\n",
    "B, T, C = x.size()\n",
    "assert(C == test_model.config.motion.dim)\n",
    "\n",
    "# Encoder: start with zero hidden states\n",
    "encoder_out, rnn_states = test_model.encoder(x)  # hidden: [num_layers, B, state_size]\n",
    "\n",
    "# Decoder initialization\n",
    "# last_input_frame = x[:, -1:, :]  # Last time step of input as initial input [B, 1, C]\n",
    "last_input_frame = encoder_out[:, -1:, :]  # Last time step of input as initial input [B, 1, C]\n",
    "decoder_input = last_input_frame.clone()\n",
    "\n",
    "output_delta_frames = torch.zeros(B, T, test_model.state_size).cuda()\n",
    "for frame_id in range(T):\n",
    "\t# Decoder input is always the last input frame\n",
    "\tframe_delta, rnn_states = test_model.decoder(decoder_input, rnn_states)\n",
    "\toutput_delta_frames[:, frame_id:frame_id+1, :] = frame_delta.clone()\n",
    "\tdecoder_input = frame_delta.clone()  # Next input is current output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 50, 99])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 66, 50])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "motion_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDCT\n",
    "motion_pred = torch.matmul(idct_m[:, :config.motion.h36m_input_length, :], motion_pred)\n",
    "\n",
    "# add residual\n",
    "if config.residual_output:\n",
    "\toffset = h36m_motion_input[:, -1:].cuda()\n",
    "\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length] + offset\n",
    "else:\n",
    "\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length]\n",
    "\n",
    "# calc losses\n",
    "b,n,c = h36m_motion_target.shape\n",
    "motion_pred = motion_pred.reshape(b,n,22,3).reshape(-1,3)\n",
    "h36m_motion_target = h36m_motion_target.cuda().reshape(b,n,22,3).reshape(-1,3)\n",
    "loss = torch.mean(torch.norm(motion_pred - h36m_motion_target, 2, 1))\n",
    "# add position loss and velocity loss\n",
    "if config.use_relative_loss:\n",
    "\tmotion_pred = motion_pred.reshape(b,n,22,3)\n",
    "\tdmotion_pred = gen_velocity(motion_pred)\n",
    "\tmotion_gt = h36m_motion_target.reshape(b,n,22,3)\n",
    "\tdmotion_gt = gen_velocity(motion_gt)\n",
    "\tdloss = torch.mean(torch.norm((dmotion_pred - dmotion_gt).reshape(-1,3), 2, 1))\n",
    "\tloss = loss + dloss\n",
    "else:\n",
    "\tloss = loss.mean()\n",
    "\n",
    "writer.add_scalar('Loss/angle', loss.detach().cpu().numpy(), nb_iter)\n",
    "\n",
    "# reset gradients\n",
    "optimizer.zero_grad()\n",
    "# compute gradients by backpropagation\n",
    "loss.backward()\n",
    "# update params\n",
    "optimizer.step()\n",
    "optimizer, current_lr = update_lr_multistep(nb_iter, total_iter, max_lr, min_lr, optimizer)\n",
    "writer.add_scalar('LR/train', current_lr, nb_iter)\n",
    "\n",
    "return loss.item(), optimizer, current_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde_675",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

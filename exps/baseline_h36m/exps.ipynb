{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Code to run in bash console\n",
    "# cd exps/baseline_h36m\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import os, sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from config import config\n",
    "\n",
    "import model as models\n",
    "from datasets.h36m import H36MDataset\n",
    "from utils.logger import get_logger, print_and_log_info\n",
    "from utils.pyt_utils import link_file, ensure_dir\n",
    "from datasets.h36m_eval import H36MEval\n",
    "\n",
    "from custom_test import test\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Run \"conda install -c conda-forge ipywidgets\" for tqdm to work in notebook\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# cuda setting to make result deterministic\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--exp-name', type=str, default=None, help='=exp name')\n",
    "parser.add_argument('--seed', type=int, default=888, help='=seed')\n",
    "parser.add_argument('--temporal-only', action='store_true', help='=temporal only')\n",
    "parser.add_argument('--layer-norm-axis', type=str, default='spatial', help='=layernorm axis')\n",
    "# default is False for 'store_true'\n",
    "parser.add_argument('--with-normalization', action='store_true', help='=use layernorm')\n",
    "parser.add_argument('--spatial-fc', action='store_true', help='=use only spatial fc')\n",
    "parser.add_argument('--num', type=int, default=64, help='=num of blocks')\n",
    "parser.add_argument('--weight', type=float, default=1., help='=loss weight')\n",
    "\n",
    "# pass argument without command line\n",
    "import shlex\n",
    "argString = '--seed 888 --exp-name baseline.txt --layer-norm-axis spatial --with-normalization --num 48'\n",
    "args = parser.parse_args(shlex.split(argString))\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "acc_log = open(args.exp_name, 'a')\n",
    "torch.manual_seed(args.seed)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "config.motion_fc_in.temporal_fc = args.temporal_only\n",
    "config.motion_fc_out.temporal_fc = args.temporal_only\n",
    "config.motion_mlp.norm_axis = args.layer_norm_axis\n",
    "config.motion_mlp.spatial_fc_only = args.spatial_fc\n",
    "config.motion_mlp.with_normalization = args.with_normalization\n",
    "config.motion_mlp.num_layers = args.num\n",
    "\n",
    "# config.motion_rnn.with_normalization = args.with_normalization\n",
    "\n",
    "acc_log.write(''.join('Seed : ' + str(args.seed) + '\\n'))\n",
    "\n",
    "def get_dct_matrix(N):\n",
    "\tdct_m = np.eye(N)\n",
    "\tfor k in np.arange(N):\n",
    "\t\tfor i in np.arange(N):\n",
    "\t\t\tw = np.sqrt(2 / N)\n",
    "\t\t\tif k == 0:\n",
    "\t\t\t\tw = np.sqrt(1 / N)\n",
    "\t\t\tdct_m[k, i] = w * np.cos(np.pi * (i + 1 / 2) * k / N)\n",
    "\tidct_m = np.linalg.inv(dct_m)\n",
    "\treturn dct_m, idct_m\n",
    "\n",
    "# size: (1,T,T)\n",
    "if config.pre_dct:\n",
    "\tdct_m,idct_m = get_dct_matrix(config.motion.h36m_input_length_dct)\n",
    "\tdct_m = torch.tensor(dct_m).float().cuda().unsqueeze(0)\n",
    "\tidct_m = torch.tensor(idct_m).float().cuda().unsqueeze(0)\n",
    "\n",
    "def update_lr_multistep(nb_iter, total_iter, max_lr, mid_lr, min_lr, optimizer):\n",
    "\tif nb_iter < 10000:\n",
    "\t\tcurrent_lr = max_lr\n",
    "\telif nb_iter < 30000:\n",
    "\t\tcurrent_lr = mid_lr\n",
    "\telse:\n",
    "\t\tcurrent_lr = min_lr\n",
    "\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\tparam_group[\"lr\"] = current_lr\n",
    "\n",
    "\treturn optimizer, current_lr\n",
    "\n",
    "def gen_velocity(m):\n",
    "\tdm = m[:, 1:] - m[:, :-1]\n",
    "\treturn dm\n",
    "\n",
    "def train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, total_iter, max_lr, mid_lr, min_lr) :\n",
    "\n",
    "\tif config.pre_dct:\n",
    "\t\tb,n,c = h36m_motion_input.shape\n",
    "\t\th36m_motion_input_ = h36m_motion_input.clone()\n",
    "\t\th36m_motion_input_ = torch.matmul(dct_m[:, :, :config.motion.h36m_input_length], h36m_motion_input_.cuda())\n",
    "\telse:\n",
    "\t\th36m_motion_input_ = h36m_motion_input.clone()\n",
    "\n",
    "\tmotion_pred = model(h36m_motion_input_.cuda())\n",
    "\n",
    "\tif config.post_dct:\n",
    "\t\tmotion_pred = torch.matmul(idct_m[:, :config.motion.h36m_input_length, :], motion_pred)\n",
    "\n",
    "\tif config.residual_output:\n",
    "\t\toffset = h36m_motion_input[:, -1:].cuda()\n",
    "\t\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length] + offset\n",
    "\telse:\n",
    "\t\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length]\n",
    "\n",
    "\t# calc losses\n",
    "\tb,n,c = h36m_motion_target.shape\n",
    "\tmotion_pred = motion_pred.reshape(b,n,22,3).reshape(-1,3)\n",
    "\th36m_motion_target = h36m_motion_target.cuda().reshape(b,n,22,3).reshape(-1,3)\n",
    "\tloss = torch.mean(torch.norm(motion_pred - h36m_motion_target, 2, 1))\n",
    "\t# add position loss and velocity loss\n",
    "\tif config.use_relative_loss:\n",
    "\t\tmotion_pred = motion_pred.reshape(b,n,22,3)\n",
    "\t\tdmotion_pred = gen_velocity(motion_pred)\n",
    "\t\tmotion_gt = h36m_motion_target.reshape(b,n,22,3)\n",
    "\t\tdmotion_gt = gen_velocity(motion_gt)\n",
    "\t\tdloss = torch.mean(torch.norm((dmotion_pred - dmotion_gt).reshape(-1,3), 2, 1))\n",
    "\t\tloss = loss + dloss\n",
    "\telse:\n",
    "\t\tloss = loss.mean()\n",
    "\n",
    "\twriter.add_scalar('Loss/angle', loss.detach().cpu().numpy(), nb_iter)\n",
    "\n",
    "\t# reset gradients\n",
    "\toptimizer.zero_grad()\n",
    "\t# compute gradients by backpropagation\n",
    "\tloss.backward()\n",
    "\t# update params\n",
    "\toptimizer.step()\n",
    "\toptimizer, current_lr = update_lr_multistep(nb_iter, total_iter, max_lr, mid_lr, min_lr, optimizer)\n",
    "\twriter.add_scalar('LR/train', current_lr, nb_iter)\n",
    "\n",
    "\treturn loss.item(), optimizer, current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlidingRNN_v2(\n",
      "  (mlp_mini): siMLPe_mini(\n",
      "    (arr0): Rearrange('b n d -> b d n')\n",
      "    (arr1): Rearrange('b d n -> b n d')\n",
      "    (motion_mlp): TransMLP(\n",
      "      (mlps): Sequential(\n",
      "        (0): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (motion_fc_out): Linear(in_features=99, out_features=66, bias=True)\n",
      "    (temporal_merge_fc): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      "  (endecoder): GRU(66, 99, batch_first=True)\n",
      "  (arr0): Rearrange('b n d -> b d n')\n",
      "  (fc_encoder): Linear(in_features=66, out_features=99, bias=True)\n",
      ")\n",
      "\n",
      "Total count of parameters: 63199\n",
      "Residual output?  False\n",
      "Use DCT?  False False\n",
      "Using recursive residual? True\n",
      "Using spatial fc before temporal in RNN? True\n",
      "Temporal layer in RNN: 1\n",
      "History term window size:  10\n",
      "Short term window size:  10\n",
      "Encode history?  True\n",
      "mlp_layers =  1\n",
      "rnn_state_size =  99\n",
      "rnn_layers =  1\n",
      "rnn_blocks =  1\n"
     ]
    }
   ],
   "source": [
    "if config.model == 'siMLPe':\n",
    "\tmodel = models.siMLPe(config)\n",
    "elif config.model == 'siMLPe_RNN':\n",
    "\tmodel = models.SlidingRNN_v2(config)\n",
    "elif config.model == 'Seq2SeqGRU':\n",
    "\tmodel = models.Seq2SeqGRU(config)\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print()\n",
    "print(\"Total count of parameters:\",total_params)\n",
    "print(\"Residual output? \",config.residual_output)\n",
    "print(\"Use DCT? \",config.pre_dct, config.post_dct)\n",
    "print(\"Using recursive residual?\",config.motion_rnn.recursive_residual)\n",
    "# print(\"Using LayerNorm?\",config.motion_rnn.with_normalization) (deprecated)\n",
    "print(\"Using spatial fc before temporal in RNN?\",config.motion_rnn.local_spatial_fc)\n",
    "print(\"Temporal layer in RNN:\",config.motion_rnn.num_temp_blocks)\n",
    "# print(\"Sliding long term encoder in RNN? \",config.motion_rnn.sliding_long_term) (deprecated)\n",
    "print(\"History term window size: \",config.motion_rnn.history_window_size)\n",
    "print(\"Short term window size: \",config.motion_rnn.short_term_window_size)\n",
    "print(\"Encode history? \",config.motion_rnn.encode_history)\n",
    "print(\"mlp_layers = \",config.motion_rnn.mlp_layers)\n",
    "print(\"rnn_state_size = \",config.motion_rnn.rnn_state_size)\n",
    "print(\"rnn_layers = \",config.motion_rnn.rnn_layers)\n",
    "print(\"rnn_blocks = \",config.motion_rnn.rnn_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/home/gjsk/siMLPe/exps/baseline_h36m/log/log_last.log': File exists\n"
     ]
    }
   ],
   "source": [
    "model.train().cuda()\n",
    "\n",
    "# dataset = (T-by-C x_in, N-by-C x_out)\n",
    "config.motion.h36m_target_length = config.motion.h36m_target_length_train\n",
    "dataset = H36MDataset(config, 'train', config.data_aug)\n",
    "\n",
    "# separate into batches (input, target) with size (batch_size,T,C) and (batch_size,N,C)\n",
    "shuffle = True\n",
    "sampler = None\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size,\n",
    "\t\t\t\t\t\tnum_workers=config.num_workers, drop_last=True,\n",
    "\t\t\t\t\t\tsampler=sampler, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "eval_config = copy.deepcopy(config)\n",
    "eval_config.motion.h36m_target_length = eval_config.motion.h36m_target_length_eval\n",
    "eval_dataset = H36MEval(eval_config, 'test')\n",
    "\n",
    "shuffle = False\n",
    "sampler = None\n",
    "# separate into batches (input, target) with size (batch_size,T=50,K,3) and (batch_size,N=25,K,3)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=128,\n",
    "\t\t\t\t\t\tnum_workers=1, drop_last=False,\n",
    "\t\t\t\t\t\tsampler=sampler, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "\t\t\t\t\t\t\t lr=config.cos_lr_max,\n",
    "\t\t\t\t\t\t\t weight_decay=config.weight_decay)\n",
    "\n",
    "ensure_dir(config.snapshot_dir)\n",
    "logger = get_logger(config.log_file, 'train')\n",
    "link_file(config.log_file, config.link_log_file)\n",
    "\n",
    "print_and_log_info(logger, json.dumps(config, indent=4, sort_keys=True))\n",
    "\n",
    "# continue training from a checkpoint\n",
    "if config.model_pth is not None:\n",
    "\tstate_dict = torch.load(config.model_pth)\n",
    "\tmodel.load_state_dict(state_dict, strict=True)\n",
    "\tprint_and_log_info(logger, \"Loading model path from {} \".format(config.model_pth))\n",
    "\tprint(\"Loading model path from {} \".format(config.model_pth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee11d6c1b5884818b5a331ba3ee9eabc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2500 results: 16.4, 32.7, 61.6, 73.4, 91.9, 105.3, 116.3, 123.3\n",
      "[5.2, 7.3, 8.9, 9.1, 8.1, 6.6, 5.3, 4.2]\n",
      "Iteration 5000 results: 15.0, 31.0, 60.7, 73.3, 93.2, 107.7, 120.4, 128.2\n",
      "[3.8, 5.6, 8.0, 9.0, 9.4, 9.0, 9.4, 9.1]\n",
      "Iteration 7500 results: 14.6, 30.5, 60.0, 72.3, 92.2, 107.3, 121.1, 130.0\n",
      "[3.4, 5.1, 7.3, 8.0, 8.4, 8.6, 10.1, 10.9]\n",
      "Iteration 10000 results: 14.2, 30.1, 59.8, 72.4, 92.5, 107.4, 120.5, 129.1\n",
      "[3.0, 4.7, 7.1, 8.1, 8.7, 8.7, 9.5, 10.0]\n",
      "Iteration 12500 results: 13.9, 29.6, 59.0, 71.4, 91.4, 106.3, 119.5, 128.2\n",
      "[2.7, 4.2, 6.3, 7.1, 7.6, 7.6, 8.5, 9.1]\n",
      "Iteration 15000 results: 13.6, 29.3, 58.9, 71.2, 90.8, 105.4, 118.0, 126.1\n",
      "[2.4, 3.9, 6.2, 6.9, 7.0, 6.7, 7.0, 7.0]\n",
      "Iteration 17500 results: 13.5, 29.4, 59.7, 72.3, 92.7, 107.9, 121.1, 129.5\n",
      "[2.3, 4.0, 7.0, 8.0, 8.9, 9.2, 10.1, 10.4]\n",
      "Iteration 20000 results: 13.3, 28.9, 58.4, 70.7, 90.2, 104.9, 117.8, 126.0\n",
      "[2.1, 3.5, 5.7, 6.4, 6.4, 6.2, 6.8, 6.9]\n",
      "Iteration 22500 results: 13.2, 29.0, 58.9, 71.3, 91.1, 105.8, 118.5, 126.5\n",
      "[2.0, 3.6, 6.2, 7.0, 7.3, 7.1, 7.5, 7.4]\n",
      "Iteration 25000 results: 13.1, 28.9, 59.0, 71.6, 91.9, 107.4, 121.3, 130.3\n",
      "[1.9, 3.5, 6.3, 7.3, 8.1, 8.7, 10.3, 11.2]\n"
     ]
    }
   ],
   "source": [
    "nb_iter = 0\n",
    "avg_loss = 0\n",
    "avg_lr = 0\n",
    "current_lr = config.cos_lr_max\n",
    "\n",
    "config.save_every = 2500\n",
    "config.cos_lr_total_iters = 40000\n",
    "baseline_results = [23.8,44.4,76.1,88.2,107.4,121.6,131.6,136.6]\n",
    "our_best_results = [11.2, 25.4, 52.7, 64.3, 83.8, 98.7, 111.0, 119.1]\n",
    "\n",
    "with tqdm(total=config.cos_lr_total_iters, desc=\"Training\") as pbar:\n",
    "\twhile (nb_iter + 1) < config.cos_lr_total_iters:\n",
    "\t\tfor (h36m_motion_input, h36m_motion_target) in dataloader:\n",
    "\n",
    "\t\t\tloss, optimizer, current_lr = train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, config.cos_lr_total_iters, config.cos_lr_max, config.cos_lr_mid, config.cos_lr_min)\n",
    "\t\t\tavg_loss += loss\n",
    "\t\t\tavg_lr += current_lr\n",
    "\n",
    "\t\t\tif (nb_iter + 1) % config.print_every ==  0 :\n",
    "\t\t\t\tavg_loss = avg_loss / config.print_every\n",
    "\t\t\t\tavg_lr = avg_lr / config.print_every\n",
    "\n",
    "\t\t\t\tprint_and_log_info(logger, \"Iter {} Summary: \".format(nb_iter + 1))\n",
    "\t\t\t\tprint_and_log_info(logger, f\"\\t lr: {avg_lr} \\t Training loss: {avg_loss}\")\n",
    "\t\t\t\tavg_loss = 0\n",
    "\t\t\t\tavg_lr = 0\n",
    "\n",
    "\t\t\tif (nb_iter + 1) % config.save_every ==  0 :\n",
    "\t\t\t\tif (nb_iter + 1) > config.cos_lr_total_iters - config.save_every - 1:\n",
    "\t\t\t\t\ttorch.save(model.state_dict(), config.snapshot_dir + '/model-iter-' + str(nb_iter + 1) + '.pth')\n",
    "\t\t\t\tmodel.eval()\n",
    "\t\t\t\tacc_tmp = test(eval_config, model, eval_dataloader)\n",
    "\t\t\t\tprint(f'Iteration {nb_iter + 1} results: {\", \".join(str(i) for i in acc_tmp)}')\n",
    "\t\t\t\tprint([round(float(acc_tmp[i]-our_best_results[i]),2) for i in range(8)])\n",
    "\t\t\t\tacc_log.write(f\"{nb_iter + 1}\\n{' '.join(str(a) for a in acc_tmp)}\\n\")\n",
    "\t\t\t\tmodel.train()\n",
    "\n",
    "\t\t\tif (nb_iter + 1) == config.cos_lr_total_iters :\n",
    "\t\t\t\tbreak\n",
    "\t\t\tnb_iter += 1\n",
    "\t\tpbar.update(nb_iter - pbar.n)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops.layers.torch import Rearrange\n",
    "arr0 = Rearrange('b n d -> b d n')\n",
    "arr1 = Rearrange('b d n -> b n d')\n",
    "\n",
    "nb_iter = 0\n",
    "avg_loss = 0\n",
    "avg_lr = 0\n",
    "\n",
    "(h36m_motion_input, h36m_motion_target) = next(iter(dataloader))\n",
    "\n",
    "# loss, optimizer, current_lr = train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, config.cos_lr_total_iters, config.cos_lr_max, config.cos_lr_min)\n",
    "# train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, total_iter, max_lr, min_lr)\n",
    "total_iter, max_lr, min_lr = config.cos_lr_total_iters, config.cos_lr_max, config.cos_lr_min\n",
    "\n",
    "# DCT\n",
    "b,n,c = h36m_motion_input.shape\n",
    "h36m_motion_input_ = h36m_motion_input.clone()\n",
    "h36m_motion_input_ = torch.matmul(dct_m[:, :, :config.motion.h36m_input_length], h36m_motion_input_.cuda())\n",
    "\n",
    "import model as models\n",
    "test_model = models.SlidingRNN_v2(config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m last_rnn_input \u001b[38;5;241m=\u001b[39m last_input_frame\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmotion_rnn\u001b[38;5;241m.\u001b[39mencode_history:\n\u001b[0;32m---> 18\u001b[0m \tencoded_history, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_input_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \twindow_history \u001b[38;5;241m=\u001b[39m test_model\u001b[38;5;241m.\u001b[39mfc_decoder(encoded_history)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m \t\u001b[38;5;66;03m# size = [B, history_window_size, test_model.config.motion_rnn.rnn_state_size]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/rnn.py:1107\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m hx[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m   1105\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor batched 3-D input, hx and cx should \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1107\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malso be 3-D but got (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhx[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mhx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-D) tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1108\u001b[0m         )\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "# motion_pred = model(h36m_motion_input_.cuda())\n",
    "x = h36m_motion_input_.cuda()\n",
    "\n",
    "B, T, C = x.size()\n",
    "assert(C == test_model.config.motion.dim)\n",
    "\n",
    "# Encoder: start with zero hidden states\n",
    "if test_model.config.motion_rnn.use_gru:\n",
    "\tencoder_out, rnn_states = test_model.endecoder(x[:,:-1,:])\n",
    "else:\n",
    "\tencoder_out, (rnn_states, cell_states) = test_model.endecoder(x[:,:-1,:])\n",
    "\n",
    "# Decoder initialization\n",
    "last_input_frame = x[:, -1:, :]  # Last time step of input as initial input [B, 1, C]\n",
    "last_rnn_input = last_input_frame.clone()\n",
    "\n",
    "if test_model.config.motion_rnn.encode_history:\n",
    "\tif self.config.motion_rnn.use_gru:\n",
    "\t\tencoded_history, _ = self.endecoder(last_input_frame, rnn_states)\n",
    "\telse:\n",
    "\t\tencoded_history, (_,_) = self.endecoder(last_input_frame, (rnn_states, cell_states))\n",
    "\twindow_history = self.fc_decoder(encoded_history)\n",
    "else:\n",
    "\t# size = [B, history_window_size, test_model.config.motion_rnn.rnn_state_size]\n",
    "\twindow_history = x[:, -test_model.config.motion_rnn.history_window_size:, :]\n",
    "\n",
    "# output_frames = torch.zeros(B, T, C).cuda()\n",
    "# for frame_id in range(T):\n",
    "# \t# Decoder: # [B, 1, C]\n",
    "# \tif test_model.config.motion_rnn.use_gru:\n",
    "# \t\tdecoder_out, rnn_states = test_model.endecoder(last_rnn_input, rnn_states)\n",
    "# \telse:\n",
    "# \t\tdecoder_out, (rnn_states, cell_states) = test_model.endecoder(last_rnn_input, (rnn_states, cell_states))\n",
    "\n",
    "# \t# decode [B,1,H] to [B,1,C]\n",
    "# \t_decoder_out = decoder_out\n",
    "# \t_decoder_out = test_model.fc_decoder(_decoder_out)\n",
    "\n",
    "# \tif test_model.config.motion_rnn.short_term_window_size > 1:\n",
    "# \t\t# generate short term window\n",
    "# \t\tframe_start_id = frame_id-(test_model.config.motion_rnn.short_term_window_size-1)\n",
    "# \t\tif frame_start_id < 0:\n",
    "# \t\t\tif frame_id == 0:\n",
    "# \t\t\t\twindow_short_term_minus_one = x[:, frame_start_id:, :]\n",
    "# \t\t\telse:\n",
    "# \t\t\t\twindow_short_term_minus_one = torch.cat([x[:, frame_start_id:, :], output_frames[:,:frame_id,:]], dim=1)\n",
    "# \t\telse:\n",
    "# \t\t\twindow_short_term_minus_one = output_frames[:, frame_start_id:frame_id, :]\n",
    "\n",
    "# \t\tmlp_input = torch.cat([window_history, window_short_term_minus_one, _decoder_out], dim=1)\n",
    "# \telse:\n",
    "# \t\tmlp_input = torch.cat([window_history, _decoder_out], dim=1)\n",
    "\t\n",
    "# \tif test_model.config.motion_rnn.recursive_residual:\n",
    "# \t\t# Residual method 1 (recursive residual; same as in 2017 Martinez paper):\n",
    "# \t\tnew_frame = test_model.mlp_mini(mlp_input) + last_rnn_input\n",
    "# \telse:\n",
    "# \t\t# Residual method 2 (residual from the last input frame):\n",
    "# \t\tnew_frame = test_model.mlp_mini(mlp_input) + last_input_frame\n",
    "\n",
    "# \toutput_frames[:, frame_id:frame_id+1, :] = new_frame\n",
    "# \t# Next input is current output\n",
    "# \tlast_rnn_input = new_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlp\n",
    "concatenated_dim = config.motion.h36m_input_length_dct+1\n",
    "test_model.mlp_mini = mlp.MLPblock(dim=config.motion_mlp.hidden_dim,seq=concatenated_dim,use_norm=config.motion_mlp.with_normalization,use_spatial_fc=config.motion_mlp.spatial_fc_only,layernorm_axis=config.motion_mlp.norm_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16896x60 and 51x51)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m motion_feats \u001b[38;5;241m=\u001b[39m test_model\u001b[38;5;241m.\u001b[39mmlp_mini\u001b[38;5;241m.\u001b[39marr0(motion_feats)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# MLP block input should be [B,C,T]\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m motion_feats \u001b[38;5;241m=\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_mini\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmotion_mlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmotion_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m motion_feats \u001b[38;5;241m=\u001b[39m test_model\u001b[38;5;241m.\u001b[39mmlp_mini\u001b[38;5;241m.\u001b[39marr1(motion_feats)\n\u001b[1;32m      8\u001b[0m motion_feats \u001b[38;5;241m=\u001b[39m test_model\u001b[38;5;241m.\u001b[39mmlp_mini\u001b[38;5;241m.\u001b[39mmotion_fc_out(motion_feats)\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/siMLPe/exps/baseline_h36m/mlp.py:110\u001b[0m, in \u001b[0;36mTransMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 110\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/siMLPe/exps/baseline_h36m/mlp.py:96\u001b[0m, in \u001b[0;36mMLPblock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 96\u001b[0m     x_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     x_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm0(x_)\n\u001b[1;32m     98\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x_\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/siMLPe/exps/baseline_h36m/mlp.py:62\u001b[0m, in \u001b[0;36mTemporal_FC.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 62\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/syde_675/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16896x60 and 51x51)"
     ]
    }
   ],
   "source": [
    "motion_feats = test_model.mlp_mini.motion_fc_in(mlp_input)\n",
    "motion_feats = test_model.mlp_mini.arr0(motion_feats)\n",
    "\n",
    "# MLP block input should be [B,C,T]\n",
    "motion_feats = test_model.mlp_mini.motion_mlp(motion_feats)\n",
    "\n",
    "motion_feats = test_model.mlp_mini.arr1(motion_feats)\n",
    "motion_feats = test_model.mlp_mini.motion_fc_out(motion_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransMLP(\n",
       "  (mlps): Sequential(\n",
       "    (0): MLPblock(\n",
       "      (fc0): Temporal_FC(\n",
       "        (fc): Linear(in_features=50, out_features=50, bias=True)\n",
       "      )\n",
       "      (norm0): LN()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.mlp_mini.motion_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDCT\n",
    "motion_pred = torch.matmul(idct_m[:, :config.motion.h36m_input_length, :], motion_pred)\n",
    "\n",
    "# add residual\n",
    "if config.residual_output:\n",
    "\toffset = h36m_motion_input[:, -1:].cuda()\n",
    "\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length] + offset\n",
    "else:\n",
    "\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length]\n",
    "\n",
    "# calc losses\n",
    "b,n,c = h36m_motion_target.shape\n",
    "motion_pred = motion_pred.reshape(b,n,22,3).reshape(-1,3)\n",
    "h36m_motion_target = h36m_motion_target.cuda().reshape(b,n,22,3).reshape(-1,3)\n",
    "loss = torch.mean(torch.norm(motion_pred - h36m_motion_target, 2, 1))\n",
    "# add position loss and velocity loss\n",
    "if config.use_relative_loss:\n",
    "\tmotion_pred = motion_pred.reshape(b,n,22,3)\n",
    "\tdmotion_pred = gen_velocity(motion_pred)\n",
    "\tmotion_gt = h36m_motion_target.reshape(b,n,22,3)\n",
    "\tdmotion_gt = gen_velocity(motion_gt)\n",
    "\tdloss = torch.mean(torch.norm((dmotion_pred - dmotion_gt).reshape(-1,3), 2, 1))\n",
    "\tloss = loss + dloss\n",
    "else:\n",
    "\tloss = loss.mean()\n",
    "\n",
    "writer.add_scalar('Loss/angle', loss.detach().cpu().numpy(), nb_iter)\n",
    "\n",
    "# reset gradients\n",
    "optimizer.zero_grad()\n",
    "# compute gradients by backpropagation\n",
    "loss.backward()\n",
    "# update params\n",
    "optimizer.step()\n",
    "optimizer, current_lr = update_lr_multistep(nb_iter, total_iter, max_lr, min_lr, optimizer)\n",
    "writer.add_scalar('LR/train', current_lr, nb_iter)\n",
    "\n",
    "return loss.item(), optimizer, current_lr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde_675",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os, sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from config import config\n",
    "from model import siMLPe as Model\n",
    "from model import siMLPe_mod as Model_mod\n",
    "from datasets.h36m import H36MDataset\n",
    "from utils.logger import get_logger, print_and_log_info\n",
    "from utils.pyt_utils import link_file, ensure_dir\n",
    "from datasets.h36m_eval import H36MEval\n",
    "\n",
    "from custom_test import test\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# cuda setting to make result deterministic\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--exp-name', type=str, default=None, help='=exp name')\n",
    "parser.add_argument('--seed', type=int, default=888, help='=seed')\n",
    "parser.add_argument('--temporal-only', action='store_true', help='=temporal only')\n",
    "parser.add_argument('--layer-norm-axis', type=str, default='spatial', help='=layernorm axis')\n",
    "# default is False for 'store_true'\n",
    "parser.add_argument('--with-normalization', action='store_true', help='=use layernorm')\n",
    "parser.add_argument('--spatial-fc', action='store_true', help='=use only spatial fc')\n",
    "parser.add_argument('--num', type=int, default=64, help='=num of blocks')\n",
    "parser.add_argument('--weight', type=float, default=1., help='=loss weight')\n",
    "\n",
    "import shlex\n",
    "argString = '--seed 888 --exp-name baseline.txt --layer-norm-axis spatial --with-normalization --num 48'\n",
    "args = parser.parse_args(shlex.split(argString))\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "acc_log = open(args.exp_name, 'a')\n",
    "torch.manual_seed(args.seed)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "config.motion_fc_in.temporal_fc = args.temporal_only\n",
    "config.motion_fc_out.temporal_fc = args.temporal_only\n",
    "config.motion_mlp.norm_axis = args.layer_norm_axis\n",
    "config.motion_mlp.spatial_fc_only = args.spatial_fc\n",
    "config.motion_mlp.with_normalization = args.with_normalization\n",
    "config.motion_mlp.num_layers = args.num\n",
    "\n",
    "acc_log.write(''.join('Seed : ' + str(args.seed) + '\\n'))\n",
    "\n",
    "def get_dct_matrix(N):\n",
    "\tdct_m = np.eye(N)\n",
    "\tfor k in np.arange(N):\n",
    "\t\tfor i in np.arange(N):\n",
    "\t\t\tw = np.sqrt(2 / N)\n",
    "\t\t\tif k == 0:\n",
    "\t\t\t\tw = np.sqrt(1 / N)\n",
    "\t\t\tdct_m[k, i] = w * np.cos(np.pi * (i + 1 / 2) * k / N)\n",
    "\tidct_m = np.linalg.inv(dct_m)\n",
    "\treturn dct_m, idct_m\n",
    "\n",
    "# size: (1,T,T)\n",
    "dct_m,idct_m = get_dct_matrix(config.motion.h36m_input_length_dct)\n",
    "dct_m = torch.tensor(dct_m).float().cuda().unsqueeze(0)\n",
    "idct_m = torch.tensor(idct_m).float().cuda().unsqueeze(0)\n",
    "\n",
    "def update_lr_multistep(nb_iter, total_iter, max_lr, min_lr, optimizer) :\n",
    "\tif nb_iter > 30000:\n",
    "\t\tcurrent_lr = 1e-5\n",
    "\telse:\n",
    "\t\tcurrent_lr = 3e-4\n",
    "\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\tparam_group[\"lr\"] = current_lr\n",
    "\n",
    "\treturn optimizer, current_lr\n",
    "\n",
    "def gen_velocity(m):\n",
    "\tdm = m[:, 1:] - m[:, :-1]\n",
    "\treturn dm\n",
    "\n",
    "def train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, total_iter, max_lr, min_lr) :\n",
    "\n",
    "\tif config.deriv_input:\n",
    "\t\tb,n,c = h36m_motion_input.shape\n",
    "\t\th36m_motion_input_ = h36m_motion_input.clone()\n",
    "\t\th36m_motion_input_ = torch.matmul(dct_m[:, :, :config.motion.h36m_input_length], h36m_motion_input_.cuda())\n",
    "\telse:\n",
    "\t\th36m_motion_input_ = h36m_motion_input.clone()\n",
    "\n",
    "\tmotion_pred = model(h36m_motion_input_.cuda())\n",
    "\tmotion_pred = torch.matmul(idct_m[:, :config.motion.h36m_input_length, :], motion_pred)\n",
    "\n",
    "\tif config.deriv_output:\n",
    "\t\toffset = h36m_motion_input[:, -1:].cuda()\n",
    "\t\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length] + offset\n",
    "\telse:\n",
    "\t\tmotion_pred = motion_pred[:, :config.motion.h36m_target_length]\n",
    "\n",
    "\t# calc losses\n",
    "\tb,n,c = h36m_motion_target.shape\n",
    "\tmotion_pred = motion_pred.reshape(b,n,22,3).reshape(-1,3)\n",
    "\th36m_motion_target = h36m_motion_target.cuda().reshape(b,n,22,3).reshape(-1,3)\n",
    "\tloss = torch.mean(torch.norm(motion_pred - h36m_motion_target, 2, 1))\n",
    "\t# add position loss and velocity loss\n",
    "\tif config.use_relative_loss:\n",
    "\t\tmotion_pred = motion_pred.reshape(b,n,22,3)\n",
    "\t\tdmotion_pred = gen_velocity(motion_pred)\n",
    "\t\tmotion_gt = h36m_motion_target.reshape(b,n,22,3)\n",
    "\t\tdmotion_gt = gen_velocity(motion_gt)\n",
    "\t\tdloss = torch.mean(torch.norm((dmotion_pred - dmotion_gt).reshape(-1,3), 2, 1))\n",
    "\t\tloss = loss + dloss\n",
    "\telse:\n",
    "\t\tloss = loss.mean()\n",
    "\n",
    "\twriter.add_scalar('Loss/angle', loss.detach().cpu().numpy(), nb_iter)\n",
    "\n",
    "\t# reset gradients\n",
    "\toptimizer.zero_grad()\n",
    "\t# compute gradients by backpropagation\n",
    "\tloss.backward()\n",
    "\t# update params\n",
    "\toptimizer.step()\n",
    "\toptimizer, current_lr = update_lr_multistep(nb_iter, total_iter, max_lr, min_lr, optimizer)\n",
    "\twriter.add_scalar('LR/train', current_lr, nb_iter)\n",
    "\n",
    "\treturn loss.item(), optimizer, current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(config)\n",
    "model = Model(config)\n",
    "model.train()\n",
    "model.cuda()\n",
    "\n",
    "# dataset = (T-by-C x_in, N-by-C x_out)\n",
    "config.motion.h36m_target_length = config.motion.h36m_target_length_train\n",
    "dataset = H36MDataset(config, 'train', config.data_aug)\n",
    "\n",
    "# separate into batches (input, target) with size (batch_size,T,C) and (batch_size,N,C)\n",
    "shuffle = True\n",
    "sampler = None\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size,\n",
    "\t\t\t\t\t\tnum_workers=config.num_workers, drop_last=True,\n",
    "\t\t\t\t\t\tsampler=sampler, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "eval_config = copy.deepcopy(config)\n",
    "eval_config.motion.h36m_target_length = eval_config.motion.h36m_target_length_eval\n",
    "eval_dataset = H36MEval(eval_config, 'test')\n",
    "\n",
    "shuffle = False\n",
    "sampler = None\n",
    "# separate into batches (input, target) with size (batch_size,T=50,K,3) and (batch_size,N=25,K,3)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=128,\n",
    "\t\t\t\t\t\tnum_workers=1, drop_last=False,\n",
    "\t\t\t\t\t\tsampler=sampler, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "\t\t\t\t\t\t\t lr=config.cos_lr_max,\n",
    "\t\t\t\t\t\t\t weight_decay=config.weight_decay)\n",
    "\n",
    "ensure_dir(config.snapshot_dir)\n",
    "logger = get_logger(config.log_file, 'train')\n",
    "link_file(config.log_file, config.link_log_file)\n",
    "\n",
    "print_and_log_info(logger, json.dumps(config, indent=4, sort_keys=True))\n",
    "\n",
    "# continue training from a checkpoint\n",
    "if config.model_pth is not None :\n",
    "\tstate_dict = torch.load(config.model_pth)\n",
    "\tmodel.load_state_dict(state_dict, strict=True)\n",
    "\tprint_and_log_info(logger, \"Loading model path from {} \".format(config.model_pth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_iter = 0\n",
    "avg_loss = 0\n",
    "avg_lr = 0\n",
    "\n",
    "# config.save_every = 5000\n",
    "# config.cos_lr_total_iters = 5000\n",
    "\n",
    "# about 4.5 min per 5000 iterations\n",
    "while (nb_iter + 1) < config.cos_lr_total_iters:\n",
    "\n",
    "\tfor (h36m_motion_input, h36m_motion_target) in dataloader:\n",
    "\n",
    "\t\tloss, optimizer, current_lr = train_step(h36m_motion_input, h36m_motion_target, model, optimizer, nb_iter, config.cos_lr_total_iters, config.cos_lr_max, config.cos_lr_min)\n",
    "\t\tavg_loss += loss\n",
    "\t\tavg_lr += current_lr\n",
    "\n",
    "\t\tif (nb_iter + 1) % config.print_every ==  0 :\n",
    "\t\t\tavg_loss = avg_loss / config.print_every\n",
    "\t\t\tavg_lr = avg_lr / config.print_every\n",
    "\n",
    "\t\t\tprint_and_log_info(logger, \"Iter {} Summary: \".format(nb_iter + 1))\n",
    "\t\t\tprint_and_log_info(logger, f\"\\t lr: {avg_lr} \\t Training loss: {avg_loss}\")\n",
    "\t\t\tavg_loss = 0\n",
    "\t\t\tavg_lr = 0\n",
    "\n",
    "\t\tif (nb_iter + 1) % config.save_every ==  0 :\n",
    "\t\t\ttorch.save(model.state_dict(), config.snapshot_dir + '/model-iter-' + str(nb_iter + 1) + '.pth')\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\tacc_tmp = test(eval_config, model, eval_dataloader)\n",
    "\t\t\tprint(acc_tmp)\n",
    "\t\t\tacc_log.write(''.join(str(nb_iter + 1) + '\\n'))\n",
    "\t\t\tline = ''\n",
    "\t\t\tfor ii in acc_tmp:\n",
    "\t\t\t\tline += str(ii) + ' '\n",
    "\t\t\tline += '\\n'\n",
    "\t\t\tacc_log.write(''.join(line))\n",
    "\t\t\tmodel.train()\n",
    "\n",
    "\t\tif (nb_iter + 1) == config.cos_lr_total_iters :\n",
    "\t\t\tbreak\n",
    "\t\tnb_iter += 1\n",
    "\tprint(nb_iter)\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde_675",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

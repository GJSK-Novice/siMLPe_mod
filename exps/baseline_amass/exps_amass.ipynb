{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9024e032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 13:04:53.439879: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-20 13:04:53.449904: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745168693.461568   26886 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745168693.464733   26886 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745168693.473399   26886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745168693.473420   26886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745168693.473422   26886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745168693.473423   26886 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-20 13:04:53.476503: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlidingRNN_v2(\n",
      "  (mlp_mini): siMLPe_mini(\n",
      "    (arr0): Rearrange('b n d -> b d n')\n",
      "    (arr1): Rearrange('b d n -> b n d')\n",
      "    (motion_mlp): TransMLP(\n",
      "      (mlps): Sequential(\n",
      "        (0): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (1): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (2): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (3): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (4): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (5): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (6): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (7): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (8): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (9): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (10): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (11): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (motion_fc_out): Linear(in_features=81, out_features=54, bias=True)\n",
      "    (temporal_merge_fc): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      "  (endecoder): GRU(54, 81, batch_first=True)\n",
      "  (arr0): Rearrange('b n d -> b d n')\n",
      "  (fc_encoder): Linear(in_features=54, out_features=81, bias=True)\n",
      ")\n",
      "\n",
      "Total count of parameters: 46003\n",
      "RNN type?  GRU\n",
      "Residual output?  False\n",
      "Use DCT?  False False\n",
      "Using recursive residual? True\n",
      "Using spatial fc before temporal in RNN? True\n",
      "Temporal layer in RNN: 1\n",
      "History term window size:  10\n",
      "Short term window size:  10\n",
      "Encode history?  True\n",
      "mlp_layers =  12\n",
      "rnn_state_size =  81\n",
      "rnn_layers =  1\n",
      "rnn_blocks =  1\n"
     ]
    }
   ],
   "source": [
    "# Code to run in bash console\n",
    "# cd exps/baseline_amass\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import os, sys\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from config import config\n",
    "import model as models\n",
    "# import exps.baseline_h36m.model as models\n",
    "from datasets.amass import AMASSDataset\n",
    "from utils.logger import get_logger, print_and_log_info\n",
    "from utils.pyt_utils import link_file, ensure_dir\n",
    "\n",
    "from custom_test import test\n",
    "from datasets.amass_eval import AMASSEval\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# cuda setting to make result deterministic\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--exp-name', type=str, default=None, help='=exp name')\n",
    "parser.add_argument('--seed', type=int, default=888, help='=seed')\n",
    "parser.add_argument('--temporal-only', action='store_true', help='=temporal only')\n",
    "parser.add_argument('--layer-norm-axis', type=str, default='spatial', help='=layernorm axis')\n",
    "# default is False for 'store_true'\n",
    "parser.add_argument('--with-normalization', action='store_true', help='=use layernorm')\n",
    "parser.add_argument('--spatial-fc', action='store_true', help='=use only spatial fc')\n",
    "parser.add_argument('--num', type=int, default=64, help='=num of blocks')\n",
    "parser.add_argument('--weight', type=float, default=1., help='=loss weight')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# pass argument without command line\n",
    "import shlex\n",
    "argString = '--seed 888 --exp-name baseline.txt --layer-norm-axis spatial --with-normalization --num 48'\n",
    "args = parser.parse_args(shlex.split(argString))\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "acc_log = open(args.exp_name, 'a')\n",
    "torch.manual_seed(args.seed)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "config.motion_fc_in.temporal_fc = args.temporal_only\n",
    "config.motion_fc_out.temporal_fc = args.temporal_only\n",
    "config.motion_mlp.norm_axis = args.layer_norm_axis\n",
    "config.motion_mlp.spatial_fc_only = args.spatial_fc\n",
    "config.motion_mlp.with_normalization = args.with_normalization\n",
    "config.motion_mlp.num_layers = args.num\n",
    "\n",
    "# config.motion_rnn.with_normalization = args.with_normalization\n",
    "\n",
    "acc_log.write(''.join('Seed : ' + str(args.seed) + '\\n'))\n",
    "\n",
    "def get_dct_matrix(N):\n",
    "\tdct_m = np.eye(N)\n",
    "\tfor k in np.arange(N):\n",
    "\t\tfor i in np.arange(N):\n",
    "\t\t\tw = np.sqrt(2 / N)\n",
    "\t\t\tif k == 0:\n",
    "\t\t\t\tw = np.sqrt(1 / N)\n",
    "\t\t\tdct_m[k, i] = w * np.cos(np.pi * (i + 1 / 2) * k / N)\n",
    "\tidct_m = np.linalg.inv(dct_m)\n",
    "\treturn dct_m, idct_m\n",
    "\n",
    "# size: (1,T,T)\n",
    "if config.pre_dct:\n",
    "\tdct_m,idct_m = get_dct_matrix(config.motion.amass_input_length_dct)\n",
    "\tdct_m = torch.tensor(dct_m).float().cuda().unsqueeze(0)\n",
    "\tidct_m = torch.tensor(idct_m).float().cuda().unsqueeze(0)\n",
    "\n",
    "def update_lr_multistep(nb_iter, total_iter, max_lr, mid_lr, min_lr, optimizer):\n",
    "\tif nb_iter < 50000:\n",
    "\t\tcurrent_lr = max_lr\n",
    "\telif nb_iter < 100000:\n",
    "\t\tcurrent_lr = mid_lr\n",
    "\telse:\n",
    "\t\tcurrent_lr = min_lr\n",
    "\n",
    "\tfor param_group in optimizer.param_groups:\n",
    "\t\tparam_group[\"lr\"] = current_lr\n",
    "\n",
    "\treturn optimizer, current_lr\n",
    "\n",
    "def gen_velocity(m):\n",
    "\tdm = m[:, 1:] - m[:, :-1]\n",
    "\treturn dm\n",
    "\n",
    "def train_step(amass_motion_input, amass_motion_target, model, optimizer, nb_iter, total_iter, max_lr, mid_lr, min_lr) :\n",
    "\n",
    "\tif config.pre_dct:\n",
    "\t\tb,n,c = amass_motion_input.shape\n",
    "\t\tamass_motion_input_ = amass_motion_input.clone()\n",
    "\t\tamass_motion_input_ = torch.matmul(dct_m, amass_motion_input_.cuda())\n",
    "\telse:\n",
    "\t\tamass_motion_input_ = amass_motion_input.clone()\n",
    "\n",
    "\tmotion_pred = model(amass_motion_input_.cuda())\n",
    "\n",
    "\tif config.post_dct:\n",
    "\t\tmotion_pred = torch.matmul(idct_m, motion_pred)\n",
    "\n",
    "\tif config.residual_output:\n",
    "\t\toffset = amass_motion_input[:, -1:].cuda()\n",
    "\t\tmotion_pred = motion_pred[:, :config.motion.amass_target_length] + offset\n",
    "\telse:\n",
    "\t\tmotion_pred = motion_pred[:, :config.motion.amass_target_length]\n",
    "\n",
    "\t# calc losses\n",
    "\tb,n,c = amass_motion_target.shape\n",
    "\tmotion_pred = motion_pred.reshape(b,n,18,3).reshape(-1,3)\n",
    "\tamass_motion_target = amass_motion_target.cuda().reshape(b,n,18,3).reshape(-1,3)\n",
    "\tloss = torch.mean(torch.norm(motion_pred - amass_motion_target, 2, 1))\n",
    "\t# add position loss and velocity loss\n",
    "\tif config.use_relative_loss:\n",
    "\t\tmotion_pred = motion_pred.reshape(b,n,18,3)\n",
    "\t\tdmotion_pred = gen_velocity(motion_pred)\n",
    "\t\tmotion_gt = amass_motion_target.reshape(b,n,18,3)\n",
    "\t\tdmotion_gt = gen_velocity(motion_gt)\n",
    "\t\tdloss = torch.mean(torch.norm((dmotion_pred - dmotion_gt).reshape(-1,3), 2, 1))\n",
    "\t\tloss = loss + dloss\n",
    "\telse:\n",
    "\t\tloss = loss.mean()\n",
    "\n",
    "\twriter.add_scalar('Loss/angle', loss.detach().cpu().numpy(), nb_iter)\n",
    "\n",
    "\t# reset gradients\n",
    "\toptimizer.zero_grad()\n",
    "\t# compute gradients by backpropagation\n",
    "\tloss.backward()\n",
    "\t# update params\n",
    "\toptimizer.step()\n",
    "\toptimizer, current_lr = update_lr_multistep(nb_iter, total_iter, max_lr, mid_lr, min_lr, optimizer)\n",
    "\twriter.add_scalar('LR/train', current_lr, nb_iter)\n",
    "\n",
    "\treturn loss.item(), optimizer, current_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74f3d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SlidingRNN_v2(\n",
      "  (mlp_mini): siMLPe_mini(\n",
      "    (arr0): Rearrange('b n d -> b d n')\n",
      "    (arr1): Rearrange('b d n -> b n d')\n",
      "    (motion_mlp): TransMLP(\n",
      "      (mlps): Sequential(\n",
      "        (0): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (1): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (2): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (3): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (4): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (5): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (6): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (7): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (8): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (9): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (10): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "        (11): MLPblock(\n",
      "          (fc0): Temporal_FC(\n",
      "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
      "          )\n",
      "          (norm0): LN()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (motion_fc_out): Linear(in_features=81, out_features=54, bias=True)\n",
      "    (temporal_merge_fc): Linear(in_features=12, out_features=1, bias=True)\n",
      "  )\n",
      "  (endecoder): GRU(54, 81, batch_first=True)\n",
      "  (arr0): Rearrange('b n d -> b d n')\n",
      "  (fc_encoder): Linear(in_features=54, out_features=81, bias=True)\n",
      ")\n",
      "\n",
      "Total count of parameters: 46003\n",
      "RNN type?  GRU\n",
      "Residual output?  False\n",
      "Use DCT?  False False\n",
      "Using recursive residual? True\n",
      "Temporal layer in RNN: 1\n",
      "History term window size:  10\n",
      "Short term window size:  10\n",
      "Encode history?  True\n",
      "mlp_layers =  12\n",
      "rnn_state_size =  81\n",
      "rnn_layers =  1\n",
      "rnn_blocks =  1\n"
     ]
    }
   ],
   "source": [
    "if config.model == 'siMLPe':\n",
    "\tmodel = models.siMLPe(config)\n",
    "elif config.model == 'siMLPe_RNN':\n",
    "\tmodel = models.SlidingRNN_v2(config)\n",
    "elif config.model == 'Seq2SeqGRU':\n",
    "\tmodel = models.Seq2SeqGRU(config)\n",
    "\n",
    "# G. Train in Graph Mode\n",
    "# Enable torch.jit.graph mode for improved computational efficiency\n",
    "# model = torch.jit.script(model)\n",
    "\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print()\n",
    "print(\"Total count of parameters:\",total_params)\n",
    "print(\"RNN type? \",\"GRU\" if config.motion_rnn.use_gru else \"LSTM\")\n",
    "print(\"Residual output? \",config.residual_output)\n",
    "print(\"Use DCT? \",config.pre_dct, config.post_dct)\n",
    "print(\"Using recursive residual?\",config.motion_rnn.recursive_residual)\n",
    "# print(\"Using LayerNorm?\",config.motion_rnn.with_normalization) (deprecated)\n",
    "# print(\"Using spatial fc before temporal in RNN?\",config.motion_rnn.local_spatial_fc) (deprecated)\n",
    "print(\"Temporal layer in RNN:\",config.motion_rnn.num_temp_blocks)\n",
    "# print(\"Sliding long term encoder in RNN? \",config.motion_rnn.sliding_long_term) (deprecated)\n",
    "print(\"History term window size: \",config.motion_rnn.history_window_size)\n",
    "print(\"Short term window size: \",config.motion_rnn.short_term_window_size)\n",
    "print(\"Encode history? \",config.motion_rnn.encode_history)\n",
    "print(\"mlp_layers = \",config.motion_rnn.mlp_layers)\n",
    "print(\"rnn_state_size = \",config.motion_rnn.rnn_state_size)\n",
    "print(\"rnn_layers = \",config.motion_rnn.rnn_layers)\n",
    "print(\"rnn_blocks = \",config.motion_rnn.rnn_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df133d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading AMASS Training Data: 100%|██████████| 3567/3567 [01:04<00:00, 54.99it/s] \n",
      "Loading AMASS Test Data: 100%|██████████| 3061/3061 [00:38<00:00, 80.27it/s] \n"
     ]
    }
   ],
   "source": [
    "model.train().cuda()\n",
    "\n",
    "# dataset = (T-by-C x_in, N-by-C x_out)\n",
    "config.motion.amass_target_length = config.motion.amass_target_length_train\n",
    "dataset = AMASSDataset(config, 'train', config.data_aug)\n",
    "\n",
    "# separate into batches (input, target) with size (batch_size,T,C) and (batch_size,N,C)\n",
    "shuffle = True\n",
    "sampler = None\n",
    "dataloader = DataLoader(dataset, batch_size=config.batch_size,\n",
    "\t\t\t\t\t\tnum_workers=config.num_workers, drop_last=True,\n",
    "\t\t\t\t\t\tsampler=sampler, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "eval_config = copy.deepcopy(config)\n",
    "eval_config.motion.amass_target_length = eval_config.motion.amass_target_length_eval\n",
    "eval_dataset = AMASSEval(eval_config, 'test')\n",
    "\n",
    "shuffle = False\n",
    "sampler = None\n",
    "# separate into batches (input, target) with size (batch_size,T=50,K,3) and (batch_size,N=25,K,3)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=128,\n",
    "\t\t\t\t\t\tnum_workers=1, drop_last=False,\n",
    "\t\t\t\t\t\tsampler=sampler, shuffle=shuffle, pin_memory=True)\n",
    "\n",
    "\n",
    "# initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "\t\t\t\t\t\t\t lr=config.cos_lr_max,\n",
    "\t\t\t\t\t\t\t weight_decay=config.weight_decay)\n",
    "\n",
    "ensure_dir(config.snapshot_dir)\n",
    "logger = get_logger(config.log_file, 'train')\n",
    "link_file(config.log_file, config.link_log_file)\n",
    "\n",
    "print_and_log_info(logger, json.dumps(config, indent=4, sort_keys=True))\n",
    "\n",
    "# continue training from a checkpoint\n",
    "if config.model_pth is not None:\n",
    "\tstate_dict = torch.load(config.model_pth)\n",
    "\tmodel.load_state_dict(state_dict, strict=True)\n",
    "\tprint_and_log_info(logger, \"Loading model path from {} \".format(config.model_pth))\n",
    "\tprint(\"Loading model path from {} \".format(config.model_pth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b084808d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training on AMASS: 100%|█████████▉| 28749/28750.0 [1:07:28<00:00,  7.10it/s]\n",
      "Testing on AMASS: 100%|██████████| 1319/1319 [02:36<00:00,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(11.644868), np.float64(23.48003), np.float64(45.811967), np.float64(55.607167), np.float64(71.823749), np.float64(83.674396), np.float64(92.201757), np.float64(97.389682)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlidingRNN_v2(\n",
       "  (mlp_mini): siMLPe_mini(\n",
       "    (arr0): Rearrange('b n d -> b d n')\n",
       "    (arr1): Rearrange('b d n -> b n d')\n",
       "    (motion_mlp): TransMLP(\n",
       "      (mlps): Sequential(\n",
       "        (0): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (1): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (2): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (3): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (4): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (5): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (6): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (7): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (8): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (9): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (10): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "        (11): MLPblock(\n",
       "          (fc0): Temporal_FC(\n",
       "            (fc): Linear(in_features=12, out_features=12, bias=True)\n",
       "          )\n",
       "          (norm0): LN()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (motion_fc_out): Linear(in_features=81, out_features=54, bias=True)\n",
       "    (temporal_merge_fc): Linear(in_features=12, out_features=1, bias=True)\n",
       "  )\n",
       "  (endecoder): GRU(54, 81, batch_first=True)\n",
       "  (arr0): Rearrange('b n d -> b d n')\n",
       "  (fc_encoder): Linear(in_features=54, out_features=81, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "nb_iter = 0\n",
    "avg_loss = 0\n",
    "avg_lr = 0\n",
    "current_lr = config.cos_lr_max\n",
    "\n",
    "# config.save_every = 2300\n",
    "config.cos_lr_total_iters = int(115000)/4\n",
    "# siMLPe_results = [10.8, 19.6, 34.3, 40.5, 50.5, 57.3, 62.4, 65.7]\n",
    "# our_best_results = [11.2, 25.1, 51.7, 63.2, 82.0, 96.3, 108.3, 116.1]\n",
    "\n",
    "with tqdm(total=config.cos_lr_total_iters, desc=\"Training on AMASS\") as pbar:\n",
    "\twhile (nb_iter + 1) < config.cos_lr_total_iters:\n",
    "\n",
    "\n",
    "\t\tfor (amass_motion_input, amass_motion_target) in dataloader:\n",
    "\n",
    "\t\t\tloss, optimizer, current_lr = train_step(amass_motion_input, amass_motion_target, model, optimizer, nb_iter, config.cos_lr_total_iters, config.cos_lr_max, config.cos_lr_mid, config.cos_lr_min)\n",
    "\t\t\tavg_loss += loss\n",
    "\t\t\tavg_lr += current_lr\n",
    "\n",
    "\t\t\tif (nb_iter + 1) % config.print_every ==  0 :\n",
    "\t\t\t\tavg_loss = avg_loss / config.print_every\n",
    "\t\t\t\tavg_lr = avg_lr / config.print_every\n",
    "\n",
    "\t\t\t\tprint_and_log_info(logger, \"Iter {} Summary: \".format(nb_iter + 1))\n",
    "\t\t\t\tprint_and_log_info(logger, f\"\\t lr: {avg_lr} \\t Training loss: {avg_loss}\")\n",
    "\t\t\t\tavg_loss = 0\n",
    "\t\t\t\tavg_lr = 0\n",
    "\n",
    "\t\t\tif (nb_iter + 1) == config.cos_lr_total_iters :\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\n",
    "\t\t\tnb_iter += 1\n",
    "\t\tpbar.update(nb_iter - pbar.n)\n",
    "torch.save(model.state_dict(), config.snapshot_dir + '/model-iter-' + str(nb_iter + 1) + '.pth')\n",
    "writer.close()\n",
    "\n",
    "# python custom_test.py --model-pth ./log/snapshot/model-iter-1000.pth\n",
    "model.eval()\n",
    "test(eval_config, model, eval_dataloader)\n",
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syde_675",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

[2025-04-20 13:07:28,479] INFO: {
    "abs_dir": "/home/gjsk/siMLPe/exps/baseline_amass",
    "amass_anno_dir": "/home/gjsk/siMLPe/data/amass/",
    "batch_size": 256,
    "cos_lr_max": 0.0005,
    "cos_lr_mid": 0.0003,
    "cos_lr_min": 1e-05,
    "cos_lr_total_iters": 115000,
    "data_aug": true,
    "link_log_file": "/home/gjsk/siMLPe/exps/baseline_amass/log/log_last.log",
    "link_val_log_file": "/home/gjsk/siMLPe/exps/baseline_amass/log/val_last.log",
    "log_dir": "/home/gjsk/siMLPe/exps/baseline_amass/log",
    "log_file": "/home/gjsk/siMLPe/exps/baseline_amass/log/log_2025_04_20_13_04_50.log",
    "model": "siMLPe_RNN",
    "model_pth": null,
    "motion": {
        "amass_input_length": 10,
        "amass_input_length_dct": 10,
        "amass_target_length": 10,
        "amass_target_length_eval": 25,
        "amass_target_length_train": 10,
        "dim": 54,
        "pw3d_input_length": 10,
        "pw3d_target_length_eval": 25,
        "pw3d_target_length_train": 10
    },
    "motion_fc_in": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": false,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_fc_out": {
        "activation": "relu",
        "in_features": 54,
        "init_w_trunc_normal": true,
        "out_features": 54,
        "temporal_fc": false,
        "with_norm": false
    },
    "motion_mlp": {
        "hidden_dim": 54,
        "norm_axis": "spatial",
        "num_layers": 48,
        "seq_len": 10,
        "spatial_fc_only": false,
        "with_normalization": true
    },
    "motion_rnn": {
        "encode_history": true,
        "history_window_size": 10,
        "local_spatial_fc": true,
        "mlp_layers": 12,
        "num_temp_blocks": 1,
        "recursive_residual": true,
        "rnn_blocks": 1,
        "rnn_layers": 1,
        "rnn_state_size": 81,
        "short_term_window_size": 10,
        "use_gru": true
    },
    "num_workers": 8,
    "post_dct": false,
    "pre_dct": false,
    "print_every": 100,
    "pw3d_anno_dir": "/home/gjsk/siMLPe/data/3dpw/sequenceFiles/",
    "repo_name": "siMLPe",
    "residual_output": false,
    "root_dir": "/home/gjsk/siMLPe",
    "save_every": 5000,
    "seed": 888,
    "shift_step": 5,
    "snapshot_dir": "/home/gjsk/siMLPe/exps/baseline_amass/log/snapshot",
    "this_dir": "baseline_amass",
    "use_relative_loss": true,
    "val_log_file": "/home/gjsk/siMLPe/exps/baseline_amass/log/val_2025_04_20_13_04_50.log",
    "weight_decay": 0.0001
}
[2025-04-20 13:07:43,883] INFO: Iter 100 Summary: 
[2025-04-20 13:07:43,884] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.06519655037671328
[2025-04-20 13:07:57,556] INFO: Iter 200 Summary: 
[2025-04-20 13:07:57,557] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.05936277162283659
[2025-04-20 13:08:11,107] INFO: Iter 300 Summary: 
[2025-04-20 13:08:11,108] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.056265346519649026
[2025-04-20 13:08:25,097] INFO: Iter 400 Summary: 
[2025-04-20 13:08:25,097] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.05541628308594227
[2025-04-20 13:08:39,102] INFO: Iter 500 Summary: 
[2025-04-20 13:08:39,103] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.05279893688857555
[2025-04-20 13:08:52,901] INFO: Iter 600 Summary: 
[2025-04-20 13:08:52,901] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.052031238563358784
[2025-04-20 13:09:06,507] INFO: Iter 700 Summary: 
[2025-04-20 13:09:06,507] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.05099576439708471
[2025-04-20 13:09:20,370] INFO: Iter 800 Summary: 
[2025-04-20 13:09:20,370] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.05101605579257011
[2025-04-20 13:09:34,253] INFO: Iter 900 Summary: 
[2025-04-20 13:09:34,254] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04953210841864347
[2025-04-20 13:09:53,096] INFO: Iter 100 Summary: 
[2025-04-20 13:09:53,097] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04913212765008211
[2025-04-20 13:10:07,224] INFO: Iter 200 Summary: 
[2025-04-20 13:10:07,224] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04806329473853111
[2025-04-20 13:10:21,151] INFO: Iter 300 Summary: 
[2025-04-20 13:10:21,151] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.047972292564809324
[2025-04-20 13:10:34,894] INFO: Iter 400 Summary: 
[2025-04-20 13:10:34,894] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04720225345343351
[2025-04-20 13:10:48,728] INFO: Iter 500 Summary: 
[2025-04-20 13:10:48,728] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04606287021189928
[2025-04-20 13:11:02,364] INFO: Iter 600 Summary: 
[2025-04-20 13:11:02,364] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04632997430860996
[2025-04-20 13:11:16,197] INFO: Iter 700 Summary: 
[2025-04-20 13:11:16,198] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04574978865683079
[2025-04-20 13:11:30,294] INFO: Iter 800 Summary: 
[2025-04-20 13:11:30,294] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04564894638955593
[2025-04-20 13:11:44,181] INFO: Iter 900 Summary: 
[2025-04-20 13:11:44,181] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04536911107599735
[2025-04-20 13:11:57,811] INFO: Iter 1000 Summary: 
[2025-04-20 13:11:57,811] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.045735574662685394
[2025-04-20 13:12:12,287] INFO: Iter 1100 Summary: 
[2025-04-20 13:12:12,287] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04501061618328094
[2025-04-20 13:12:26,925] INFO: Iter 1200 Summary: 
[2025-04-20 13:12:26,925] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.044797933213412765
[2025-04-20 13:12:40,865] INFO: Iter 1300 Summary: 
[2025-04-20 13:12:40,865] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04455008685588837
[2025-04-20 13:12:54,402] INFO: Iter 1400 Summary: 
[2025-04-20 13:12:54,402] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.044514666125178334
[2025-04-20 13:13:07,945] INFO: Iter 1500 Summary: 
[2025-04-20 13:13:07,946] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04412198856472969
[2025-04-20 13:13:22,155] INFO: Iter 1600 Summary: 
[2025-04-20 13:13:22,156] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04414647053927183
[2025-04-20 13:13:35,874] INFO: Iter 1700 Summary: 
[2025-04-20 13:13:35,874] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04407645646482706
[2025-04-20 13:13:49,802] INFO: Iter 1800 Summary: 
[2025-04-20 13:13:49,802] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04390536028891802
[2025-04-20 13:14:03,676] INFO: Iter 1900 Summary: 
[2025-04-20 13:14:03,676] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04386868067085743
[2025-04-20 13:14:16,788] INFO: Iter 2000 Summary: 
[2025-04-20 13:14:16,789] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0440736286714673
[2025-04-20 13:14:30,227] INFO: Iter 2100 Summary: 
[2025-04-20 13:14:30,227] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.043611681833863256
[2025-04-20 13:14:43,800] INFO: Iter 2200 Summary: 
[2025-04-20 13:14:43,801] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04352688279002905
[2025-04-20 13:14:57,238] INFO: Iter 2300 Summary: 
[2025-04-20 13:14:57,239] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04346940122544765
[2025-04-20 13:15:10,691] INFO: Iter 2400 Summary: 
[2025-04-20 13:15:10,692] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0435893863812089
[2025-04-20 13:15:24,644] INFO: Iter 2500 Summary: 
[2025-04-20 13:15:24,645] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04308565769344568
[2025-04-20 13:15:39,477] INFO: Iter 2600 Summary: 
[2025-04-20 13:15:39,477] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04284344516694546
[2025-04-20 13:15:54,207] INFO: Iter 2700 Summary: 
[2025-04-20 13:15:54,207] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04357896067202091
[2025-04-20 13:16:09,557] INFO: Iter 2800 Summary: 
[2025-04-20 13:16:09,558] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04303116288036108
[2025-04-20 13:16:24,822] INFO: Iter 2900 Summary: 
[2025-04-20 13:16:24,823] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04279850997030735
[2025-04-20 13:16:39,475] INFO: Iter 3000 Summary: 
[2025-04-20 13:16:39,475] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04295103222131729
[2025-04-20 13:16:54,113] INFO: Iter 3100 Summary: 
[2025-04-20 13:16:54,114] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04295148245990276
[2025-04-20 13:17:08,865] INFO: Iter 3200 Summary: 
[2025-04-20 13:17:08,865] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.042621956542134284
[2025-04-20 13:17:23,237] INFO: Iter 3300 Summary: 
[2025-04-20 13:17:23,238] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.042759153507649895
[2025-04-20 13:17:37,968] INFO: Iter 3400 Summary: 
[2025-04-20 13:17:37,968] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04283688224852085
[2025-04-20 13:17:52,885] INFO: Iter 3500 Summary: 
[2025-04-20 13:17:52,885] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04245753418654204
[2025-04-20 13:18:07,488] INFO: Iter 3600 Summary: 
[2025-04-20 13:18:07,488] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04304074168205261
[2025-04-20 13:18:22,101] INFO: Iter 3700 Summary: 
[2025-04-20 13:18:22,101] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04237016014754772
[2025-04-20 13:18:36,587] INFO: Iter 3800 Summary: 
[2025-04-20 13:18:36,587] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.042392459362745286
[2025-04-20 13:18:50,409] INFO: Iter 3900 Summary: 
[2025-04-20 13:18:50,410] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.042067374996840955
[2025-04-20 13:19:04,521] INFO: Iter 4000 Summary: 
[2025-04-20 13:19:04,522] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04210815452039242
[2025-04-20 13:19:17,966] INFO: Iter 4100 Summary: 
[2025-04-20 13:19:17,967] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041987632364034654
[2025-04-20 13:19:31,983] INFO: Iter 4200 Summary: 
[2025-04-20 13:19:31,983] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041945221796631814
[2025-04-20 13:19:45,346] INFO: Iter 4300 Summary: 
[2025-04-20 13:19:45,346] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04214028764516115
[2025-04-20 13:19:58,879] INFO: Iter 4400 Summary: 
[2025-04-20 13:19:58,879] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04257857538759709
[2025-04-20 13:20:12,775] INFO: Iter 4500 Summary: 
[2025-04-20 13:20:12,775] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04211068663746118
[2025-04-20 13:20:26,292] INFO: Iter 4600 Summary: 
[2025-04-20 13:20:26,293] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041710263192653654
[2025-04-20 13:20:40,886] INFO: Iter 4700 Summary: 
[2025-04-20 13:20:40,886] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04174602124840021
[2025-04-20 13:20:55,178] INFO: Iter 4800 Summary: 
[2025-04-20 13:20:55,178] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04208936735987663
[2025-04-20 13:21:08,721] INFO: Iter 4900 Summary: 
[2025-04-20 13:21:08,722] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04178553111851215
[2025-04-20 13:21:22,742] INFO: Iter 5000 Summary: 
[2025-04-20 13:21:22,742] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04179820165038109
[2025-04-20 13:21:36,476] INFO: Iter 5100 Summary: 
[2025-04-20 13:21:36,476] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04149886403232813
[2025-04-20 13:21:50,250] INFO: Iter 5200 Summary: 
[2025-04-20 13:21:50,251] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041834323704242705
[2025-04-20 13:22:04,256] INFO: Iter 5300 Summary: 
[2025-04-20 13:22:04,256] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04176810096949339
[2025-04-20 13:22:18,772] INFO: Iter 5400 Summary: 
[2025-04-20 13:22:18,773] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0414499918371439
[2025-04-20 13:22:33,550] INFO: Iter 5500 Summary: 
[2025-04-20 13:22:33,550] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041460131257772447
[2025-04-20 13:22:47,148] INFO: Iter 5600 Summary: 
[2025-04-20 13:22:47,149] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04151065748184919
[2025-04-20 13:23:01,113] INFO: Iter 5700 Summary: 
[2025-04-20 13:23:01,114] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04110155828297138
[2025-04-20 13:23:15,445] INFO: Iter 5800 Summary: 
[2025-04-20 13:23:15,445] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04146957132965326
[2025-04-20 13:23:30,046] INFO: Iter 5900 Summary: 
[2025-04-20 13:23:30,046] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04163762934505939
[2025-04-20 13:23:44,300] INFO: Iter 6000 Summary: 
[2025-04-20 13:23:44,301] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04120898462831974
[2025-04-20 13:23:58,306] INFO: Iter 6100 Summary: 
[2025-04-20 13:23:58,306] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04119308102875948
[2025-04-20 13:24:12,341] INFO: Iter 6200 Summary: 
[2025-04-20 13:24:12,342] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041338246464729306
[2025-04-20 13:24:27,219] INFO: Iter 6300 Summary: 
[2025-04-20 13:24:27,220] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04121243931353092
[2025-04-20 13:24:41,230] INFO: Iter 6400 Summary: 
[2025-04-20 13:24:41,231] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04133907601237297
[2025-04-20 13:24:55,054] INFO: Iter 6500 Summary: 
[2025-04-20 13:24:55,055] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040834919176995756
[2025-04-20 13:25:09,018] INFO: Iter 6600 Summary: 
[2025-04-20 13:25:09,018] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04130788866430521
[2025-04-20 13:25:23,192] INFO: Iter 6700 Summary: 
[2025-04-20 13:25:23,193] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04129688948392868
[2025-04-20 13:25:36,955] INFO: Iter 6800 Summary: 
[2025-04-20 13:25:36,955] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04123593896627426
[2025-04-20 13:25:50,692] INFO: Iter 6900 Summary: 
[2025-04-20 13:25:50,693] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04060921471565962
[2025-04-20 13:26:04,406] INFO: Iter 7000 Summary: 
[2025-04-20 13:26:04,406] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041047218330204485
[2025-04-20 13:26:17,734] INFO: Iter 7100 Summary: 
[2025-04-20 13:26:17,735] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04076585076749325
[2025-04-20 13:26:30,822] INFO: Iter 7200 Summary: 
[2025-04-20 13:26:30,823] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04099242385476828
[2025-04-20 13:26:43,955] INFO: Iter 7300 Summary: 
[2025-04-20 13:26:43,956] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041003912910819056
[2025-04-20 13:26:57,509] INFO: Iter 7400 Summary: 
[2025-04-20 13:26:57,509] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040608766637742516
[2025-04-20 13:27:11,519] INFO: Iter 7500 Summary: 
[2025-04-20 13:27:11,520] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04096369761973619
[2025-04-20 13:27:25,033] INFO: Iter 7600 Summary: 
[2025-04-20 13:27:25,034] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040655809044837954
[2025-04-20 13:27:38,608] INFO: Iter 7700 Summary: 
[2025-04-20 13:27:38,608] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04071198560297489
[2025-04-20 13:27:52,695] INFO: Iter 7800 Summary: 
[2025-04-20 13:27:52,695] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0407345949485898
[2025-04-20 13:28:06,408] INFO: Iter 7900 Summary: 
[2025-04-20 13:28:06,408] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.041143262796103953
[2025-04-20 13:28:20,550] INFO: Iter 8000 Summary: 
[2025-04-20 13:28:20,550] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04061075169593096
[2025-04-20 13:28:34,526] INFO: Iter 8100 Summary: 
[2025-04-20 13:28:34,526] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04072725635021925
[2025-04-20 13:28:47,881] INFO: Iter 8200 Summary: 
[2025-04-20 13:28:47,882] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04054871514439583
[2025-04-20 13:29:01,937] INFO: Iter 8300 Summary: 
[2025-04-20 13:29:01,938] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04094602424651384
[2025-04-20 13:29:15,674] INFO: Iter 8400 Summary: 
[2025-04-20 13:29:15,674] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04075176402926445
[2025-04-20 13:29:29,033] INFO: Iter 8500 Summary: 
[2025-04-20 13:29:29,034] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04058001291006803
[2025-04-20 13:29:43,011] INFO: Iter 8600 Summary: 
[2025-04-20 13:29:43,012] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04094917193055153
[2025-04-20 13:29:57,088] INFO: Iter 8700 Summary: 
[2025-04-20 13:29:57,088] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04066363979130983
[2025-04-20 13:30:10,769] INFO: Iter 8800 Summary: 
[2025-04-20 13:30:10,770] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04080743782222271
[2025-04-20 13:30:25,850] INFO: Iter 8900 Summary: 
[2025-04-20 13:30:25,851] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04062106590718031
[2025-04-20 13:30:40,061] INFO: Iter 9000 Summary: 
[2025-04-20 13:30:40,062] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040489840433001516
[2025-04-20 13:30:54,383] INFO: Iter 9100 Summary: 
[2025-04-20 13:30:54,384] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040471610613167286
[2025-04-20 13:31:08,222] INFO: Iter 9200 Summary: 
[2025-04-20 13:31:08,222] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04062826544046402
[2025-04-20 13:31:21,842] INFO: Iter 9300 Summary: 
[2025-04-20 13:31:21,843] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04052745938301086
[2025-04-20 13:31:35,878] INFO: Iter 9400 Summary: 
[2025-04-20 13:31:35,879] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04038071528077126
[2025-04-20 13:31:48,962] INFO: Iter 9500 Summary: 
[2025-04-20 13:31:48,963] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040125456638634205
[2025-04-20 13:32:02,548] INFO: Iter 9600 Summary: 
[2025-04-20 13:32:02,549] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04076199132949114
[2025-04-20 13:32:16,345] INFO: Iter 9700 Summary: 
[2025-04-20 13:32:16,346] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04068025648593902
[2025-04-20 13:32:30,137] INFO: Iter 9800 Summary: 
[2025-04-20 13:32:30,137] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04052594833076
[2025-04-20 13:32:44,271] INFO: Iter 9900 Summary: 
[2025-04-20 13:32:44,272] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0400867049023509
[2025-04-20 13:32:58,594] INFO: Iter 10000 Summary: 
[2025-04-20 13:32:58,594] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04016011200845242
[2025-04-20 13:33:12,510] INFO: Iter 10100 Summary: 
[2025-04-20 13:33:12,511] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040003989338874814
[2025-04-20 13:33:26,568] INFO: Iter 10200 Summary: 
[2025-04-20 13:33:26,569] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04026055477559567
[2025-04-20 13:33:41,010] INFO: Iter 10300 Summary: 
[2025-04-20 13:33:41,010] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04041640534996986
[2025-04-20 13:33:55,745] INFO: Iter 10400 Summary: 
[2025-04-20 13:33:55,745] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04045710943639278
[2025-04-20 13:34:10,589] INFO: Iter 10500 Summary: 
[2025-04-20 13:34:10,590] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04003234501928091
[2025-04-20 13:34:25,593] INFO: Iter 10600 Summary: 
[2025-04-20 13:34:25,594] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04026146739721298
[2025-04-20 13:34:39,969] INFO: Iter 10700 Summary: 
[2025-04-20 13:34:39,969] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04018280241638422
[2025-04-20 13:34:54,234] INFO: Iter 10800 Summary: 
[2025-04-20 13:34:54,235] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040295027159154416
[2025-04-20 13:35:08,477] INFO: Iter 10900 Summary: 
[2025-04-20 13:35:08,478] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03997700735926628
[2025-04-20 13:35:22,641] INFO: Iter 11000 Summary: 
[2025-04-20 13:35:22,641] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0405550866574049
[2025-04-20 13:35:36,595] INFO: Iter 11100 Summary: 
[2025-04-20 13:35:36,595] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039975516721606257
[2025-04-20 13:35:50,822] INFO: Iter 11200 Summary: 
[2025-04-20 13:35:50,822] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040063754990696906
[2025-04-20 13:36:05,403] INFO: Iter 11300 Summary: 
[2025-04-20 13:36:05,404] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040443326756358144
[2025-04-20 13:36:19,622] INFO: Iter 11400 Summary: 
[2025-04-20 13:36:19,622] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04003807693719864
[2025-04-20 13:36:34,386] INFO: Iter 11500 Summary: 
[2025-04-20 13:36:34,386] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04003100726753473
[2025-04-20 13:36:48,429] INFO: Iter 11600 Summary: 
[2025-04-20 13:36:48,429] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040231410413980484
[2025-04-20 13:37:02,201] INFO: Iter 11700 Summary: 
[2025-04-20 13:37:02,201] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040154614448547364
[2025-04-20 13:37:16,248] INFO: Iter 11800 Summary: 
[2025-04-20 13:37:16,249] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04005225267261267
[2025-04-20 13:37:30,745] INFO: Iter 11900 Summary: 
[2025-04-20 13:37:30,746] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04008708830922842
[2025-04-20 13:37:45,363] INFO: Iter 12000 Summary: 
[2025-04-20 13:37:45,363] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03955223761498928
[2025-04-20 13:37:59,225] INFO: Iter 12100 Summary: 
[2025-04-20 13:37:59,226] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04007445011287927
[2025-04-20 13:38:13,833] INFO: Iter 12200 Summary: 
[2025-04-20 13:38:13,834] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03986736960709095
[2025-04-20 13:38:28,226] INFO: Iter 12300 Summary: 
[2025-04-20 13:38:28,227] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.040146676786243916
[2025-04-20 13:38:42,204] INFO: Iter 12400 Summary: 
[2025-04-20 13:38:42,205] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04017743993550539
[2025-04-20 13:38:55,963] INFO: Iter 12500 Summary: 
[2025-04-20 13:38:55,964] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04019844923168421
[2025-04-20 13:39:10,160] INFO: Iter 12600 Summary: 
[2025-04-20 13:39:10,161] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03985648531466723
[2025-04-20 13:39:23,763] INFO: Iter 12700 Summary: 
[2025-04-20 13:39:23,763] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03970672175288201
[2025-04-20 13:39:37,922] INFO: Iter 12800 Summary: 
[2025-04-20 13:39:37,922] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0402726436778903
[2025-04-20 13:39:52,273] INFO: Iter 12900 Summary: 
[2025-04-20 13:39:52,273] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03968645025044679
[2025-04-20 13:40:06,646] INFO: Iter 13000 Summary: 
[2025-04-20 13:40:06,647] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03985789854079485
[2025-04-20 13:40:21,098] INFO: Iter 13100 Summary: 
[2025-04-20 13:40:21,099] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03950278423726559
[2025-04-20 13:40:35,230] INFO: Iter 13200 Summary: 
[2025-04-20 13:40:35,230] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0400179810449481
[2025-04-20 13:40:49,774] INFO: Iter 13300 Summary: 
[2025-04-20 13:40:49,774] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03978332672268152
[2025-04-20 13:41:03,806] INFO: Iter 13400 Summary: 
[2025-04-20 13:41:03,806] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03996791634708643
[2025-04-20 13:41:18,252] INFO: Iter 13500 Summary: 
[2025-04-20 13:41:18,252] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.04017122108489275
[2025-04-20 13:41:32,105] INFO: Iter 13600 Summary: 
[2025-04-20 13:41:32,105] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039857085794210434
[2025-04-20 13:41:45,743] INFO: Iter 13700 Summary: 
[2025-04-20 13:41:45,743] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03991664830595255
[2025-04-20 13:42:00,087] INFO: Iter 13800 Summary: 
[2025-04-20 13:42:00,088] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03968991812318563
[2025-04-20 13:42:14,810] INFO: Iter 13900 Summary: 
[2025-04-20 13:42:14,811] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039991824179887774
[2025-04-20 13:42:29,752] INFO: Iter 14000 Summary: 
[2025-04-20 13:42:29,753] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039996896684169767
[2025-04-20 13:42:43,851] INFO: Iter 14100 Summary: 
[2025-04-20 13:42:43,860] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039735189154744145
[2025-04-20 13:42:57,643] INFO: Iter 14200 Summary: 
[2025-04-20 13:42:57,643] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0398830808699131
[2025-04-20 13:43:11,492] INFO: Iter 14300 Summary: 
[2025-04-20 13:43:11,493] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039597151204943656
[2025-04-20 13:43:25,954] INFO: Iter 14400 Summary: 
[2025-04-20 13:43:25,954] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039666975140571596
[2025-04-20 13:43:39,771] INFO: Iter 14500 Summary: 
[2025-04-20 13:43:39,771] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0394892542809248
[2025-04-20 13:43:53,632] INFO: Iter 14600 Summary: 
[2025-04-20 13:43:53,632] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03990290876477957
[2025-04-20 13:44:06,877] INFO: Iter 14700 Summary: 
[2025-04-20 13:44:06,877] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03978481221944094
[2025-04-20 13:44:20,384] INFO: Iter 14800 Summary: 
[2025-04-20 13:44:20,384] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039664962366223334
[2025-04-20 13:44:34,052] INFO: Iter 14900 Summary: 
[2025-04-20 13:44:34,052] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0395359380915761
[2025-04-20 13:44:47,447] INFO: Iter 15000 Summary: 
[2025-04-20 13:44:47,448] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03996334407478571
[2025-04-20 13:45:00,960] INFO: Iter 15100 Summary: 
[2025-04-20 13:45:00,960] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03943586874753237
[2025-04-20 13:45:14,270] INFO: Iter 15200 Summary: 
[2025-04-20 13:45:14,271] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039671228677034376
[2025-04-20 13:45:27,469] INFO: Iter 15300 Summary: 
[2025-04-20 13:45:27,469] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03938545361161232
[2025-04-20 13:45:41,055] INFO: Iter 15400 Summary: 
[2025-04-20 13:45:41,055] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03959950216114521
[2025-04-20 13:45:55,306] INFO: Iter 15500 Summary: 
[2025-04-20 13:45:55,306] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03964242037385702
[2025-04-20 13:46:09,017] INFO: Iter 15600 Summary: 
[2025-04-20 13:46:09,017] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0397570188716054
[2025-04-20 13:46:23,404] INFO: Iter 15700 Summary: 
[2025-04-20 13:46:23,404] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039305609315633774
[2025-04-20 13:46:37,213] INFO: Iter 15800 Summary: 
[2025-04-20 13:46:37,213] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039984985515475274
[2025-04-20 13:46:51,225] INFO: Iter 15900 Summary: 
[2025-04-20 13:46:51,225] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039448338635265826
[2025-04-20 13:47:05,071] INFO: Iter 16000 Summary: 
[2025-04-20 13:47:05,071] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03947704255580902
[2025-04-20 13:47:18,952] INFO: Iter 16100 Summary: 
[2025-04-20 13:47:18,952] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03960181634873152
[2025-04-20 13:47:32,603] INFO: Iter 16200 Summary: 
[2025-04-20 13:47:32,603] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0397355042770505
[2025-04-20 13:47:46,535] INFO: Iter 16300 Summary: 
[2025-04-20 13:47:46,536] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039203908890485764
[2025-04-20 13:48:00,166] INFO: Iter 16400 Summary: 
[2025-04-20 13:48:00,166] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039661057628691196
[2025-04-20 13:48:16,374] INFO: Iter 16500 Summary: 
[2025-04-20 13:48:16,374] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039544208869338036
[2025-04-20 13:48:30,415] INFO: Iter 16600 Summary: 
[2025-04-20 13:48:30,416] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03939915895462036
[2025-04-20 13:48:44,375] INFO: Iter 16700 Summary: 
[2025-04-20 13:48:44,375] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039696316421031955
[2025-04-20 13:48:58,326] INFO: Iter 16800 Summary: 
[2025-04-20 13:48:58,326] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039410317577421666
[2025-04-20 13:49:12,126] INFO: Iter 16900 Summary: 
[2025-04-20 13:49:12,127] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03936662968248129
[2025-04-20 13:49:26,132] INFO: Iter 17000 Summary: 
[2025-04-20 13:49:26,132] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03929928503930569
[2025-04-20 13:49:39,991] INFO: Iter 17100 Summary: 
[2025-04-20 13:49:39,991] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039766162037849426
[2025-04-20 13:49:53,850] INFO: Iter 17200 Summary: 
[2025-04-20 13:49:53,850] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039642882011830805
[2025-04-20 13:50:07,334] INFO: Iter 17300 Summary: 
[2025-04-20 13:50:07,335] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0397670541703701
[2025-04-20 13:50:21,038] INFO: Iter 17400 Summary: 
[2025-04-20 13:50:21,038] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03959284923970699
[2025-04-20 13:50:34,718] INFO: Iter 17500 Summary: 
[2025-04-20 13:50:34,719] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03956835895776749
[2025-04-20 13:50:48,176] INFO: Iter 17600 Summary: 
[2025-04-20 13:50:48,177] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03925289854407311
[2025-04-20 13:51:02,031] INFO: Iter 17700 Summary: 
[2025-04-20 13:51:02,031] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03937718097120523
[2025-04-20 13:51:15,646] INFO: Iter 17800 Summary: 
[2025-04-20 13:51:15,647] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03982795249670744
[2025-04-20 13:51:29,097] INFO: Iter 17900 Summary: 
[2025-04-20 13:51:29,098] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03930566288530826
[2025-04-20 13:51:43,777] INFO: Iter 18000 Summary: 
[2025-04-20 13:51:43,778] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03934689439833164
[2025-04-20 13:51:58,613] INFO: Iter 18100 Summary: 
[2025-04-20 13:51:58,613] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03907265100628138
[2025-04-20 13:52:13,147] INFO: Iter 18200 Summary: 
[2025-04-20 13:52:13,148] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03897643871605396
[2025-04-20 13:52:28,136] INFO: Iter 18300 Summary: 
[2025-04-20 13:52:28,137] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03949006512761116
[2025-04-20 13:52:42,808] INFO: Iter 18400 Summary: 
[2025-04-20 13:52:42,808] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03927240200340748
[2025-04-20 13:52:57,852] INFO: Iter 18500 Summary: 
[2025-04-20 13:52:57,853] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039189782328903675
[2025-04-20 13:53:12,510] INFO: Iter 18600 Summary: 
[2025-04-20 13:53:12,510] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03917459942400456
[2025-04-20 13:53:26,728] INFO: Iter 18700 Summary: 
[2025-04-20 13:53:26,728] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03899527620524168
[2025-04-20 13:53:41,129] INFO: Iter 18800 Summary: 
[2025-04-20 13:53:41,130] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03974893849343061
[2025-04-20 13:53:55,633] INFO: Iter 18900 Summary: 
[2025-04-20 13:53:55,633] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039306255131959913
[2025-04-20 13:54:10,503] INFO: Iter 19000 Summary: 
[2025-04-20 13:54:10,504] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039520218446850774
[2025-04-20 13:54:24,747] INFO: Iter 19100 Summary: 
[2025-04-20 13:54:24,747] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03900131981819868
[2025-04-20 13:54:38,590] INFO: Iter 19200 Summary: 
[2025-04-20 13:54:38,591] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03894759692251682
[2025-04-20 13:54:52,749] INFO: Iter 19300 Summary: 
[2025-04-20 13:54:52,749] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039246988743543626
[2025-04-20 13:55:06,800] INFO: Iter 19400 Summary: 
[2025-04-20 13:55:06,801] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039147208333015444
[2025-04-20 13:55:21,094] INFO: Iter 19500 Summary: 
[2025-04-20 13:55:21,094] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0390387899056077
[2025-04-20 13:55:35,513] INFO: Iter 19600 Summary: 
[2025-04-20 13:55:35,513] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0390614422224462
[2025-04-20 13:55:49,395] INFO: Iter 19700 Summary: 
[2025-04-20 13:55:49,396] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03876821383833885
[2025-04-20 13:56:03,612] INFO: Iter 19800 Summary: 
[2025-04-20 13:56:03,612] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03923854403197766
[2025-04-20 13:56:17,457] INFO: Iter 19900 Summary: 
[2025-04-20 13:56:17,458] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0388911509886384
[2025-04-20 13:56:31,707] INFO: Iter 20000 Summary: 
[2025-04-20 13:56:31,708] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03971722815185785
[2025-04-20 13:56:45,750] INFO: Iter 20100 Summary: 
[2025-04-20 13:56:45,750] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03905234821140766
[2025-04-20 13:56:59,767] INFO: Iter 20200 Summary: 
[2025-04-20 13:56:59,767] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03912838365882635
[2025-04-20 13:57:14,029] INFO: Iter 20300 Summary: 
[2025-04-20 13:57:14,030] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03892806973308325
[2025-04-20 13:57:28,045] INFO: Iter 20400 Summary: 
[2025-04-20 13:57:28,045] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038617359399795534
[2025-04-20 13:57:42,193] INFO: Iter 20500 Summary: 
[2025-04-20 13:57:42,193] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03918124310672283
[2025-04-20 13:57:56,827] INFO: Iter 20600 Summary: 
[2025-04-20 13:57:56,828] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03869409203529358
[2025-04-20 13:58:11,267] INFO: Iter 20700 Summary: 
[2025-04-20 13:58:11,267] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039219481609761714
[2025-04-20 13:58:25,268] INFO: Iter 20800 Summary: 
[2025-04-20 13:58:25,268] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03882601927965879
[2025-04-20 13:58:39,849] INFO: Iter 20900 Summary: 
[2025-04-20 13:58:39,850] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03912181511521339
[2025-04-20 13:58:54,450] INFO: Iter 21000 Summary: 
[2025-04-20 13:58:54,450] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03890510905534029
[2025-04-20 13:59:08,997] INFO: Iter 21100 Summary: 
[2025-04-20 13:59:08,998] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039248090721666816
[2025-04-20 13:59:23,216] INFO: Iter 21200 Summary: 
[2025-04-20 13:59:23,217] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03906434971839189
[2025-04-20 13:59:37,740] INFO: Iter 21300 Summary: 
[2025-04-20 13:59:37,740] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03889823742210865
[2025-04-20 13:59:52,414] INFO: Iter 21400 Summary: 
[2025-04-20 13:59:52,414] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03899228498339653
[2025-04-20 14:00:06,872] INFO: Iter 21500 Summary: 
[2025-04-20 14:00:06,872] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03895891673862934
[2025-04-20 14:00:21,099] INFO: Iter 21600 Summary: 
[2025-04-20 14:00:21,100] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03865977298468351
[2025-04-20 14:00:35,515] INFO: Iter 21700 Summary: 
[2025-04-20 14:00:35,515] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03901823207736015
[2025-04-20 14:00:49,561] INFO: Iter 21800 Summary: 
[2025-04-20 14:00:49,562] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03932389471679926
[2025-04-20 14:01:03,872] INFO: Iter 21900 Summary: 
[2025-04-20 14:01:03,872] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039329087622463704
[2025-04-20 14:01:17,848] INFO: Iter 22000 Summary: 
[2025-04-20 14:01:17,848] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03886818386614323
[2025-04-20 14:01:32,133] INFO: Iter 22100 Summary: 
[2025-04-20 14:01:32,133] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03912333428859711
[2025-04-20 14:01:46,239] INFO: Iter 22200 Summary: 
[2025-04-20 14:01:46,240] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03930823985487222
[2025-04-20 14:02:00,614] INFO: Iter 22300 Summary: 
[2025-04-20 14:02:00,614] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038786223977804186
[2025-04-20 14:02:14,968] INFO: Iter 22400 Summary: 
[2025-04-20 14:02:14,969] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038661359176039696
[2025-04-20 14:02:28,684] INFO: Iter 22500 Summary: 
[2025-04-20 14:02:28,684] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03915023189038038
[2025-04-20 14:02:42,953] INFO: Iter 22600 Summary: 
[2025-04-20 14:02:42,953] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038959082998335365
[2025-04-20 14:02:56,791] INFO: Iter 22700 Summary: 
[2025-04-20 14:02:56,791] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039210543259978296
[2025-04-20 14:03:10,552] INFO: Iter 22800 Summary: 
[2025-04-20 14:03:10,552] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03888744976371527
[2025-04-20 14:03:24,436] INFO: Iter 22900 Summary: 
[2025-04-20 14:03:24,436] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03866709604859352
[2025-04-20 14:03:38,498] INFO: Iter 23000 Summary: 
[2025-04-20 14:03:38,498] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039035122692584995
[2025-04-20 14:03:52,195] INFO: Iter 23100 Summary: 
[2025-04-20 14:03:52,195] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038943795412778856
[2025-04-20 14:04:06,239] INFO: Iter 23200 Summary: 
[2025-04-20 14:04:06,239] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038968578316271305
[2025-04-20 14:04:20,204] INFO: Iter 23300 Summary: 
[2025-04-20 14:04:20,204] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03883153673261404
[2025-04-20 14:04:33,624] INFO: Iter 23400 Summary: 
[2025-04-20 14:04:33,624] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039084156043827534
[2025-04-20 14:04:47,243] INFO: Iter 23500 Summary: 
[2025-04-20 14:04:47,243] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03887426983565092
[2025-04-20 14:05:00,920] INFO: Iter 23600 Summary: 
[2025-04-20 14:05:00,920] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03867468565702439
[2025-04-20 14:05:15,043] INFO: Iter 23700 Summary: 
[2025-04-20 14:05:15,044] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03888163398951292
[2025-04-20 14:05:28,569] INFO: Iter 23800 Summary: 
[2025-04-20 14:05:28,569] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03887093890458346
[2025-04-20 14:05:42,125] INFO: Iter 23900 Summary: 
[2025-04-20 14:05:42,125] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03905169285833836
[2025-04-20 14:05:56,302] INFO: Iter 24000 Summary: 
[2025-04-20 14:05:56,302] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03904312871396542
[2025-04-20 14:06:10,614] INFO: Iter 24100 Summary: 
[2025-04-20 14:06:10,614] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03914919070899486
[2025-04-20 14:06:25,994] INFO: Iter 24200 Summary: 
[2025-04-20 14:06:25,995] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03898539882153273
[2025-04-20 14:06:40,042] INFO: Iter 24300 Summary: 
[2025-04-20 14:06:40,042] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03848981712013483
[2025-04-20 14:06:53,882] INFO: Iter 24400 Summary: 
[2025-04-20 14:06:53,882] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03912007998675108
[2025-04-20 14:07:07,962] INFO: Iter 24500 Summary: 
[2025-04-20 14:07:07,963] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03895510260015726
[2025-04-20 14:07:22,263] INFO: Iter 24600 Summary: 
[2025-04-20 14:07:22,264] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03870059188455343
[2025-04-20 14:07:35,329] INFO: Iter 24700 Summary: 
[2025-04-20 14:07:35,330] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03891088631004095
[2025-04-20 14:07:49,156] INFO: Iter 24800 Summary: 
[2025-04-20 14:07:49,157] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038897599838674066
[2025-04-20 14:08:02,511] INFO: Iter 24900 Summary: 
[2025-04-20 14:08:02,511] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038836715817451475
[2025-04-20 14:08:16,082] INFO: Iter 25000 Summary: 
[2025-04-20 14:08:16,083] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03881786528974771
[2025-04-20 14:08:29,533] INFO: Iter 25100 Summary: 
[2025-04-20 14:08:29,534] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03864511936903
[2025-04-20 14:08:43,726] INFO: Iter 25200 Summary: 
[2025-04-20 14:08:43,727] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03905373066663742
[2025-04-20 14:08:57,617] INFO: Iter 25300 Summary: 
[2025-04-20 14:08:57,617] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03868279859423637
[2025-04-20 14:09:11,509] INFO: Iter 25400 Summary: 
[2025-04-20 14:09:11,509] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03875138215720653
[2025-04-20 14:09:25,452] INFO: Iter 25500 Summary: 
[2025-04-20 14:09:25,453] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039034360349178315
[2025-04-20 14:09:39,580] INFO: Iter 25600 Summary: 
[2025-04-20 14:09:39,580] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0386995068565011
[2025-04-20 14:09:53,965] INFO: Iter 25700 Summary: 
[2025-04-20 14:09:53,966] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03898121997714043
[2025-04-20 14:10:08,802] INFO: Iter 25800 Summary: 
[2025-04-20 14:10:08,803] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038876852579414846
[2025-04-20 14:10:23,000] INFO: Iter 25900 Summary: 
[2025-04-20 14:10:23,000] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0385868825763464
[2025-04-20 14:10:36,854] INFO: Iter 26000 Summary: 
[2025-04-20 14:10:36,854] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038842691145837306
[2025-04-20 14:10:51,053] INFO: Iter 26100 Summary: 
[2025-04-20 14:10:51,053] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03841560762375593
[2025-04-20 14:11:05,185] INFO: Iter 26200 Summary: 
[2025-04-20 14:11:05,185] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03891216557472944
[2025-04-20 14:11:19,402] INFO: Iter 26300 Summary: 
[2025-04-20 14:11:19,402] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03876498840749264
[2025-04-20 14:11:33,899] INFO: Iter 26400 Summary: 
[2025-04-20 14:11:33,900] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038622397407889364
[2025-04-20 14:11:48,342] INFO: Iter 26500 Summary: 
[2025-04-20 14:11:48,343] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03874945349991322
[2025-04-20 14:12:03,333] INFO: Iter 26600 Summary: 
[2025-04-20 14:12:03,333] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0385499532148242
[2025-04-20 14:12:17,728] INFO: Iter 26700 Summary: 
[2025-04-20 14:12:17,729] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0390552457422018
[2025-04-20 14:12:32,539] INFO: Iter 26800 Summary: 
[2025-04-20 14:12:32,540] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03859579704701901
[2025-04-20 14:12:46,674] INFO: Iter 26900 Summary: 
[2025-04-20 14:12:46,674] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038951665610074994
[2025-04-20 14:13:00,638] INFO: Iter 27000 Summary: 
[2025-04-20 14:13:00,639] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.039068167321383955
[2025-04-20 14:13:14,787] INFO: Iter 27100 Summary: 
[2025-04-20 14:13:14,787] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03873795796185732
[2025-04-20 14:13:29,036] INFO: Iter 27200 Summary: 
[2025-04-20 14:13:29,037] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03858506113290787
[2025-04-20 14:13:42,631] INFO: Iter 27300 Summary: 
[2025-04-20 14:13:42,631] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038697204068303105
[2025-04-20 14:13:56,930] INFO: Iter 27400 Summary: 
[2025-04-20 14:13:56,930] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03843449510633946
[2025-04-20 14:14:10,714] INFO: Iter 27500 Summary: 
[2025-04-20 14:14:10,715] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03878218904137611
[2025-04-20 14:14:24,683] INFO: Iter 27600 Summary: 
[2025-04-20 14:14:24,683] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03860888320952654
[2025-04-20 14:14:38,242] INFO: Iter 27700 Summary: 
[2025-04-20 14:14:38,242] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03860858809202909
[2025-04-20 14:14:52,362] INFO: Iter 27800 Summary: 
[2025-04-20 14:14:52,363] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038824541680514815
[2025-04-20 14:15:06,631] INFO: Iter 27900 Summary: 
[2025-04-20 14:15:06,631] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03883459512144327
[2025-04-20 14:15:20,477] INFO: Iter 28000 Summary: 
[2025-04-20 14:15:20,478] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03871001083403826
[2025-04-20 14:15:34,262] INFO: Iter 28100 Summary: 
[2025-04-20 14:15:34,262] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03885562267154455
[2025-04-20 14:15:48,981] INFO: Iter 28200 Summary: 
[2025-04-20 14:15:48,981] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03890520308166742
[2025-04-20 14:16:03,225] INFO: Iter 28300 Summary: 
[2025-04-20 14:16:03,225] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03879581149667501
[2025-04-20 14:16:17,535] INFO: Iter 28400 Summary: 
[2025-04-20 14:16:17,535] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.0388515217974782
[2025-04-20 14:16:31,907] INFO: Iter 28500 Summary: 
[2025-04-20 14:16:31,907] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.038753931820392606
[2025-04-20 14:16:46,237] INFO: Iter 28600 Summary: 
[2025-04-20 14:16:46,238] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03865835398435593
[2025-04-20 14:17:00,169] INFO: Iter 28700 Summary: 
[2025-04-20 14:17:00,170] INFO: 	 lr: 0.0005000000000000003 	 Training loss: 0.03882711846381426
